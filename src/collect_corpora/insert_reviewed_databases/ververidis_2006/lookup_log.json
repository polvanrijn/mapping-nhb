{"AbelinAllwood2000": false, "Alter2000": false, "Amir2000": false, "Ang2002": false, "Batliner2004": false, "Choukri2003": false, "ChuangWu2002": false, "Clavel2004": false, "Fischer1999": false, "France2000": false, "Gonzalez1999": false, "Hansen1996": false, "Linnankoski2005": false, "MakarovaPetrushin2002": false, "Martins1998": false, "McMahon2003": false, "Montanari2004": false, "MozziconacciHermes1997": false, "Nwe2003": false, "Pereira2000": false, "Petrushin1999": false, "PolzinWaibel2000": false, "PolzinWaibel1998": false, "RahurkarHansen2002": false, "Scherer2000": false, "Scherer2002": false, "Schiel2002": false, "Schr\u00f6der2000": {"names": "Schrober, M", "full_names": "Schrober, M", "title": "Experimental study of affect bursts", "journal": "SPEECH COMMUNICATION", "language": "English", "publication_type": "Article", "author_keywords": "affect bursts; interjections; emotion", "keywords_plus": "EMOTION RECOGNITION; SPEAKER AFFECT; VOCAL EMOTION; EXPRESSION", "abstract": "The study described here investigates the perceived emotional content of \"affect bursts\" for German. Affect bursts are defined as short emotional non-speech expressions. This study shows that affect bursts, presented without context, can convey a clearly identifiable emotional meaning. The influence of the segmental structure on emotion recognition, as opposed to prosody and voice quality, is investigated. Agreement between transcribers is used as an experimental criterion for distinguishing between reflexive raw affect bursts and conventionalised. affect emblems. A detailed account of 28 affect burst classes is given, including perceived emotion and recognition rate in listening and reading perception tests as well as a phonetic transcription of segmental structure, voice quality and intonation. (C) 2002 Elsevier Science B.V. All rights reserved.", "affiliation": "DFKI, D-66123 Saarbrucken, Germany; Univ Saarland, Inst Phonet, D-6600 Saarbrucken, Germany", "author_information": "Schrober, M (reprint author), DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbrucken, Germany.", "email": "schroed@dfki.de", "citation_count_WOS": 37, "wos_usage_count_180": 1, "wos_usage_count_2013": 3, "journal_abbr": "Speech Commun.", "month_day": "APR", "year": 2003, "volume": 40, "issue": "1-2", "category1": "Acoustics; Computer Science, Interdisciplinary Applications", "category2": "Acoustics; Computer Science", "DOI": "10.1016/S0167-6393(02)00078-X"}, "Stibbard2000": false, "WendtScheich2002": false, "Yildirim2004": false, "Yu2001": {"names": "Yu, F; Chang, E; Xu, YQ; Shum, HY", "full_names": "Yu, F; Chang, E; Xu, YQ; Shum, HY", "title": "Emotion detection from speech to enrich multimedia content", "journal": "ADVANCES IN MUTLIMEDIA INFORMATION PROCESSING - PCM 2001, PROCEEDINGS", "language": "English", "publication_type": "Article; Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "This paper describes an experimental study on the detection of emotion from speech. As computer-based characters such as avatars and virtual chat faces become more common, the use of emotion to drive the expression of the virtual characters becomes more important. This study utilizes a corpus containing emotional speech with 721 short utterances expressing four emotions: anger, happiness, sadness, and the neutral (unemotional) state, which were captured manually from movies and teleplays. We introduce a new concept to evaluate emotions in speech. Emotions are so complex that most speech sentences cannot be precisely assigned to a particular emotion category; however, most emotional states nevertheless can be described as a mixture of multiple emotions, Based on this concept we have trained SVMs (support vector machines) to recognize utterances within these four categories and developed an agent that can recognize and express emotions.", "affiliation": "Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; Microsoft Res China, Beijing Sigma Ctr F3, Beijing 100080, Peoples R China", "author_information": "Yu, F (reprint author), Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.", "email": NaN, "citation_count_WOS": 14, "wos_usage_count_180": 0, "wos_usage_count_2013": 4, "journal_abbr": NaN, "month_day": NaN, "year": 2001, "volume": 2195, "issue": NaN, "category1": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods", "category2": "Computer Science", "DOI": NaN}, "Yuan2002": false, "Ambrus2000": false, "Bulut2002": false, "BurkhardtSendlmeier2000": false, "Caldognetto2004": false, "Edgington1997": false, "EngbergHansen1996": false, "Heuft1996": {"names": "Heuft, B; Portele, T; Rauth, M", "full_names": "Heuft, B; Portele, T; Rauth, M", "title": "Emotions in time domain synthesis", "journal": "ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "A preliminary test exploring 4 emotions showed that conveying emotions by time domain synthesis may be possible. Therefore, a mon sophisticated test was carried out in order to determine the influence of the prosodic parameters in the perception of a speaker's emotional state. Six different emotional states were investigated. The stimuli of the second test were used in three different testing procedures: as natural speech, resynthesized and reduced to a sawtooth signal. The recognition rates were lower than in the preliminary test, although the differences between the recognition rates of natural and synthetic speech wen comparable for both tests. The outcome of the sawtooth test showed that the amount of information about a speaker's emotional state transported by F-0, energy and overall duration is rather small. However, we could determine relations between the acoustic prosodic parameters and the emotional content of speech.", "affiliation": NaN, "author_information": "Heuft, B (reprint author), LERNOUT & HAUSPIE SPEECH PROD,IEPER,BELGIUM.", "email": NaN, "citation_count_WOS": 0, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 1996, "volume": NaN, "issue": NaN, "category1": "Acoustics; Engineering, Electrical & Electronic; Language & Linguistics; Psychology, Experimental", "category2": "Acoustics; Engineering; Linguistics; Psychology", "DOI": NaN}, "Iida2000": false, "Iriondo2000": false, "Kawanami2003": false, "Montero1999": false, "Niimi2001": false, "Nordstrand2004": false, "Schr\u00f6derGrice2003": false, "Tato2002": false, "Cole2005": false}