{"EngbergHansen1996": false, "Montero1999": false, "Amir2000": false, "Pereira2000": false, "Iriondo2000": false, "Nogueiras2001": false, "New2001": false, "Yu2001": {"names": "Yu, F; Chang, E; Xu, YQ; Shum, HY", "full_names": "Yu, F; Chang, E; Xu, YQ; Shum, HY", "title": "Emotion detection from speech to enrich multimedia content", "journal": "ADVANCES IN MUTLIMEDIA INFORMATION PROCESSING - PCM 2001, PROCEEDINGS", "language": "English", "publication_type": "Article; Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "This paper describes an experimental study on the detection of emotion from speech. As computer-based characters such as avatars and virtual chat faces become more common, the use of emotion to drive the expression of the virtual characters becomes more important. This study utilizes a corpus containing emotional speech with 721 short utterances expressing four emotions: anger, happiness, sadness, and the neutral (unemotional) state, which were captured manually from movies and teleplays. We introduce a new concept to evaluate emotions in speech. Emotions are so complex that most speech sentences cannot be precisely assigned to a particular emotion category; however, most emotional states nevertheless can be described as a mixture of multiple emotions, Based on this concept we have trained SVMs (support vector machines) to recognize utterances within these four categories and developed an agent that can recognize and express emotions.", "affiliation": "Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; Microsoft Res China, Beijing Sigma Ctr F3, Beijing 100080, Peoples R China", "author_information": "Yu, F (reprint author), Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.", "email": NaN, "citation_count_WOS": 14, "wos_usage_count_180": 0, "wos_usage_count_2013": 4, "journal_abbr": NaN, "month_day": NaN, "year": 2001, "volume": 2195, "issue": NaN, "category1": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods", "category2": "Computer Science", "DOI": NaN}, "MakarovaPetrushin2002": false, "Bulut2002": false, "Scherer2002": false, "Tato2002": false, "ChuangWu2002": false, "Yuan2002": false, "Hozjan2002": false, "RahurkarHansen2002": false, "SchroderGrice2003": false, "LeeNarayanan2003": false, "Yamagishi2003": false, "Schuller2003": false, "HozjanKacic2003": false, "Jovi\u010di\u01072004": false, "Schuller2004": false, "Batliner2004": false, "Yildirim2004": false, "JiangCai2004": false, "Ververidis2004": false, "Jiang2005": {"names": "Jiang, DN; Zhang, W; Shen, LQ; Cai, LH", "full_names": "Jiang, DN; Zhang, W; Shen, LQ; Cai, LH", "title": "Prosody analysis and modeling for emotional speech synthesis", "journal": "2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOLS 1-5: SPEECH PROCESSING", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "Current concatenative Text-to-Speech systems can synthesize varied emotions, but the subtle and range of the results are limited because large amount of emotional speech data are required. This paper studies a more flexible approach based on analyzing and modeling the emotional prosody features. Perceptual tests are first performed to investigate whether just manipulating prosody features can attain the communication purposes of emotions. Then, based on the positive results, the same corpus with sufficient prosody coverage is shared by different emotions in unit selection. Finally, an adaptation algorithm is proposed to predict the emotional prosody features. It models the prosodic variations by linguistic cues and emotion cues separately, and requires only a small amount of data. Experiments on Mandarin show that the adaptation algorithm can obtain appropriate emotional prosody features, and at least several emotions can be synthesized without the use of special emotional corpus.", "affiliation": NaN, "author_information": NaN, "email": "jdn00@mails.tsinghua.edu.cn; zhangzw@cn.ibm.com; shenlq@cn.ibm.com; clh-dcs@tsinghua.edu.cn", "citation_count_WOS": 8, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "category2": "Computer Science; Engineering", "DOI": NaN}, "CichoszSlot2005": false, "LinWei2005": {"names": "Lin, YL; Wei, G", "full_names": "Lin, YL; Wei, G", "title": "Speech emotion recognition based on HMM and SVM", "journal": "PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion recognition; Hidden Markov Model; Support Vector Machine; Sequential Forward Selection; Mel energy; spectrum dynamics coefficients", "keywords_plus": NaN, "abstract": "Automatic emotion recognition in speech is a current research area with a wide range of applications in human-machine interactions. This paper uses two classification methods, the hidden Markov model (HMM) and the support vector machine (SVM), to classify five emotional states: anger, happiness, sadness, surprise and a neutral state. In the HMM method, 39 candidate instantaneous features were extracted, and the Sequential Forward Selection (SFS) method was used to find the best feature subset. The classification performance of the selected feature subset was then compared with that of the Mel frequency cepstrum coefficients (MFCC). Within the method based on SVM, a new vector measuring the difference between Mel frequency scale sub-bands energies is proposed. The performance of the K-nearest Neighbors (KNN) classifier using the proposed vector was also investigated. Both gender dependent and gender independent experiments were conducted on the Danish Emotional Speech (DES) Database. The recognition rates by the HMM classifier were 98.9% for female subjects, 100% for male subjects, and 99.5% for gender independent cases. When the SVM classifier and the proposed feature vector were employed, correct classification rates of 89.4%, 93.6% and 88.9% were obtained for male, female and gender independent cases respectively.", "affiliation": "S China Univ Technol, Sch Elect & Informat Engn, Guangzhou 5106410, Peoples R China", "author_information": "Lin, YL (reprint author), S China Univ Technol, Sch Elect & Informat Engn, Guangzhou 5106410, Peoples R China.", "email": "linyilin2000@163.com; ecgwei@scut.edu.cn", "citation_count_WOS": 9, "wos_usage_count_180": 0, "wos_usage_count_2013": 2, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Information Systems", "category2": "Computer Science", "DOI": NaN}, "Luengo2005": false, "Pao2005": {"names": "Pao, TL; Chen, YT; Yeh, JH; Liao, WY", "full_names": "Pao, TL; Chen, YT; Yeh, JH; Liao, WY", "title": "Combining acoustic features for improved emotion recognition in Mandarin speech", "journal": "AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PROCEEDINGS", "language": "English", "publication_type": "Article; Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "Combining different feature streams to obtain a more accurate experimental result is a well-known technique. The basic argument is that if the recognition errors of systems using the individual streams occur at different points, there is at least a chance that a combined system will be able to correct some of these errors by reference to the other streams. In the emotional speech recognition system, there are many ways in which this general principle can be applied. In this paper, we proposed using feature selection and feature combination to improve the speaker-dependent emotion recognition in Mandarin speech. Five basic emotions are investigated including anger, boredom, happiness, neutral and sadness. Combining multiple feature streams is clearly highly beneficial in our system. The best accuracy recognizing five different emotions can be achieved 99.44% by using MFCC, LPCC, RastaPLP, LFPC feature streams and the nearest class mean classifier.", "affiliation": "Tatung Univ, Dept Comp Sci & Engn, Taipei, Taiwan", "author_information": "Pao, TL (reprint author), Tatung Univ, Dept Comp Sci & Engn, Taipei, Taiwan.", "email": "tlpao@ttu.edu.tw", "citation_count_WOS": 9, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": 3784, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "category2": "Computer Science", "DOI": NaN}, "Batliner2006": false, "Wu2006": false, "Grimm2006": false, "Kandali2008": {"names": "Kandali, AB; Routray, A; Basu, TK", "full_names": "Kandali, Aditya Bihar; Routray, Aurobinda; Basu, Tapan Kumar", "title": "Emotion recognition from Assamese speeches using MFCC features and GMM classifier", "journal": "2008 IEEE REGION 10 CONFERENCE: TENCON 2008, VOLS 1-4", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "GMM classifier; MFCC; Full-blown emotion; Simulated and Induced emotions", "keywords_plus": NaN, "abstract": "This paper presents a method based on Gaussian mixture model (GMM) classifier and Mel-frequency cepstral coefficients (MFCC) as features for emotion recognition from Assamese speeches. For training and testing of the method, data collection is carried out in Jorhat (Assam, India), which consisted of acted speeches of one short emotionally biased sentence repeated 5 times with different styles by 27 speakers (14 Male and 13 Female) for training and one long emotional speech by each speaker for testing. The experiments are performed for the cases of (i) text-independent but speaker-dependent and (ii) text-independent and speaker-independent.", "affiliation": "[Kandali, Aditya Bihar; Routray, Aurobinda; Basu, Tapan Kumar] Indian Inst Technol, Dept Elect Engn, Kharagpur 721302, W Bengal, India", "author_information": "Kandali, AB (reprint author), Indian Inst Technol, Dept Elect Engn, Kharagpur 721302, W Bengal, India.", "email": "abkandali@rediffmail.com; aroutray@iitkgp.ac.in; basutk02@yahoo.co.in", "citation_count_WOS": 16, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2008, "volume": NaN, "issue": NaN, "category1": "Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic; Telecommunications", "category2": "Computer Science; Engineering; Telecommunications", "DOI": NaN}, "Grimm2008": false, "Koolagudi2009": false, "MohantySwain2010": false, "RaoKoolagudi2011": false, "Caballero-Morales2013": {"names": "Caballero-Morales, SO", "full_names": "Caballero-Morales, Santiago-Omar", "title": "Recognition of Emotions in Mexican Spanish Speech: An Approach Based on Acoustic Modelling of Emotion-Specific Vowels", "journal": "SCIENTIFIC WORLD JOURNAL", "language": "English", "publication_type": "Article", "author_keywords": NaN, "keywords_plus": "CLASSIFICATION; STRESS", "abstract": "An approach for the recognition of emotions in speech is presented. The target language is Mexican Spanish, and for this purpose a speech database was created. The approach consists in the phoneme acoustic modelling of emotion-specific vowels. For this, a standard phoneme-based Automatic Speech Recognition (ASR) system was built with Hidden Markov Models (HMMs), where different phoneme HMMs were built for the consonants and emotion-specific vowels associated with four emotional states (anger, happiness, neutral, sadness). Then, estimation of the emotional state from a spoken sentence is performed by counting the number of emotion-specific vowels found in the ASR's output for the sentence. With this approach, accuracy of 87-100% was achieved for the recognition of emotional state of Mexican Spanish speech.", "affiliation": "Technol Univ Mixteca, Huajuapan De Leon 69000, OAX, Mexico", "author_information": "Caballero-Morales, SO (reprint author), Technol Univ Mixteca, Rd Acatlima Km 2-5, Huajuapan De Leon 69000, OAX, Mexico.", "email": "scaballero@mixteco.utm.mx", "citation_count_WOS": 38, "wos_usage_count_180": 0, "wos_usage_count_2013": 9, "journal_abbr": "Sci. World J.", "month_day": NaN, "year": 2013, "volume": NaN, "issue": NaN, "category1": "Multidisciplinary Sciences", "category2": "Science & Technology - Other Topics", "DOI": "10.1155/2013/162093"}, "Quiros-Ramirez2014": false, "EsmaileyanMarvi2014": false, "Kadiri2015": {"names": "Kadiri, SR; Gangamohan, P; Gangashetty, SV; Yegnanarayana, B", "full_names": "Kadiri, Sudarsana Reddy; Gangamohan, P.; Gangashetty, Suryakanth V.; Yegnanarayana, B.", "title": "Analysis of Excitation Source Features of Speech for Emotion Recognition", "journal": "16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "Emotion recognition; Linear prediction analysis; Zero frequency filtering; Kullback-Leibler distance", "keywords_plus": "FUNDAMENTAL-FREQUENCY", "abstract": "During production of emotional speech there are deviations in the components of speech production mechanism when compared to normal speech. The objective of this study is to capture the deviations in features related to the excitation source component of speech, and to develop a system for automatic recognition of emotions based on these deviations. The emotions considered for this study are: anger, happy, neutral and sad. The study shows that there are useful features in the deviations of the excitation source features at subsegmental level, and they can be exploited to develop an emotion recognition system. A hierarchical binary decision tree approach is used for classification.", "affiliation": "[Kadiri, Sudarsana Reddy; Gangamohan, P.; Gangashetty, Suryakanth V.; Yegnanarayana, B.] Int Inst Informat Technol, Speech & Vis Lab, Hyderabad, Andhra Pradesh, India", "author_information": "Kadiri, SR (reprint author), Int Inst Informat Technol, Speech & Vis Lab, Hyderabad, Andhra Pradesh, India.", "email": "sudarsanareddy.kadiri@research.iiit.ac.in; gangamohan.p@students.iiit.ac.in; svg@iiit.ac.in; yegna@iiit.ac.in", "citation_count_WOS": 29, "wos_usage_count_180": 0, "wos_usage_count_2013": 2, "journal_abbr": NaN, "month_day": NaN, "year": 2015, "volume": NaN, "issue": NaN, "category1": "Acoustics; Computer Science, Interdisciplinary Applications", "category2": "Acoustics; Computer Science", "DOI": NaN}, "Song2016": {"names": "Song, P; Ou, SF; Zheng, WM; Jin, Y; Zhao, L", "full_names": "Song, Peng; Ou, Shifeng; Zheng, Wenming; Jin, Yun; Zhao, Li", "title": "SPEECH EMOTION RECOGNITION USING TRANSFER NON-NEGATIVE MATRIX FACTORIZATION", "journal": "2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "Speech emotion recognition; transfer learning; non-negative matrix factorization; maximum mean discrepancy", "keywords_plus": "FEATURES", "abstract": "In practical situations, the emotional speech utterances are often collected from different devices and conditions, which will obviously affect the recognition performance. To address this issue, in this paper, a novel transfer non-negative matrix factorization (TNMF) method is presented for cross-corpus speech emotion recognition. First, the NMF algorithm is adopted to learn a latent common feature space for the source and target datasets. Then, the discrepancies between the feature distributions of different corpora are considered, and the maximum mean discrepancy (MMD) algorithm is used for the similarity measurement. Finally, the TNMF approach, which integrates the NMF and MMD algorithms, is proposed. Experiments are carried out on two popular datasets, and the results verify that the TNMF method can significantly outperform the automatic and competitive methods for cross-corpus speech emotion recognition.", "affiliation": "[Song, Peng; Ou, Shifeng] Yantai Univ, Sch Comp & Control Engn, Yantai 264005, Peoples R China; [Song, Peng; Zheng, Wenming; Jin, Yun; Zhao, Li] Southeast Univ, Minist Educ, Key Lab Child Dev & Learning Sci, Nanjing 210096, Jiangsu, Peoples R China", "author_information": "Zheng, WM (reprint author), Southeast Univ, Minist Educ, Key Lab Child Dev & Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.", "email": "pengsong@seu.edu.cn; wenming_zheng@seu.edu.cn", "citation_count_WOS": 20, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2016, "volume": NaN, "issue": NaN, "category1": "Acoustics; Engineering, Electrical & Electronic", "category2": "Acoustics; Engineering", "DOI": NaN}, "Brester2016": {"names": "Brester, C; Semenkin, E; Sidorov, M", "full_names": "Brester, Christina; Semenkin, Eugene; Sidorov, Maxim", "title": "MULTI-OBJECTIVE HEURISTIC FEATURE SELECTION FOR SPEECH-BASED MULTILINGUAL EMOTION RECOGNITION", "journal": "JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH", "language": "English", "publication_type": "Article", "author_keywords": "multi-objective optimization; feature selection; speech-based emotion recognition", "keywords_plus": NaN, "abstract": "If conventional feature selection methods do not show sufficient effectiveness, alternative algorithmic schemes might be used. In this paper we propose an evolutionary feature selection technique based on the two-criterion optimization model. To diminish the drawbacks of genetic algorithms, which are applied as optimizers, we design a parallel multi-criteria heuristic procedure based on an island model. The performance of the proposed approach was investigated on the Speech-based Emotion Recognition Problem, which reflects one of the most essential points in the sphere of human-machine communications. A number of multilingual corpora (German, English and Japanese) were involved in the experiments. According to the results obtained, a high level of emotion recognition was achieved (up to a 12.97% relative improvement compared with the best F-score value on the full set of attributes).", "affiliation": "[Brester, Christina; Semenkin, Eugene] Reshetnev Siberian State Aerosp Univ, Inst Comp Sci & Telecommun, Krasnoyarsky Rabochy Ave 31, Krasnoyarsk 660037, Russia; [Sidorov, Maxim] Ulm Univ, Inst Commun Engn, Albert Einstein Allee 43, D-89081 Ulm, Germany", "author_information": "Brester, C (reprint author), Reshetnev Siberian State Aerosp Univ, Inst Comp Sci & Telecommun, Krasnoyarsky Rabochy Ave 31, Krasnoyarsk 660037, Russia.", "email": NaN, "citation_count_WOS": 20, "wos_usage_count_180": 0, "wos_usage_count_2013": 2, "journal_abbr": "J. Artif. Intell. Soft Comput. Res.", "month_day": "OCT", "year": 2016, "volume": 6, "issue": 4, "category1": "Computer Science, Artificial Intelligence", "category2": "Computer Science", "DOI": "10.1515/jaiscr-2016-0018"}}