reference	language	actedness	subjects	goal	emotions
Engberg, Hansen, 1996	Danish	Simulated	Four actors, two of each gender (two isolated words, nine sentences and two passages)	Synthesis To evaluate how well the emotional state in emotional speech is identified by humans	Anger, sadness, surprise, neutral, happiness
Montero, 1999	Spanish	Simulated	Two sessions conducted one professional actor, (3 passages, 15 sentences of neutral-content text). 2000 phonemes per emotion are considered for analysis	Synthesis Pitch, tempo, and stress are used for synthesis	Happiness, sadness, cold anger, surprise
Amir, 2000	Hebrew	Natural	40 subjects were considered (19 males, 21 females)	Physiologic evaluations Signal analysis over sliding windows and extracting a representative feature set	Anger, fear, joy, sadness, disgust
Pereira, 2000	English	Simulated	2 actors (40 utterances)	Recognition Findings of emotions on the three dimensional scales arousal, pleasure and power	Happiness, sadness, hot anger, cold anger, neutral
Schröder, 2000	German	Simulated	6 native speakers (3 male, 3 female)	Recognition	Admiration, fear, disgust, elation, boredom, relief, startle, worry, contempt, anger
Iriondo, 2000	Spanish	Simulated	Eight actors (4 male and 4 female), 336 discourses were recorded	Synthesis	Desire, disgust, fear, anger, joy, sadness, surprise
Nogueiras, 2001	Spanish	Simulated	Two professional actors (one male and one female)	Synthesis Emotion recognition using RAMES, the UOC’s speech recognition system based on standard speech recognition technology using hidden semi-continous Markov models	Anger, disgust, fear, joy, sadness, surprise, neutral
New, 2001	Burmese	Simulated or acted	Two Burmese language speakers, 90 emotional utterances each from two speakers	Recognition A universal codebook is constructed based on emotions	Anger, dislike, fear, happiness, sadness, surprise
Yu, 2001	Chinese	Simulated	Native TV actors 721 short utterances per emotion are recorded	Recognition	Anger, happiness, neutral, sad
Makarova, Petrushin, 2002	Russian	Simulated	61 Native speakers (12 male, 49 female), 10 sentences were recorded per emotion, total 3660 utterances	Recognition This database is a source for linguistic and speech processing research	Neutral, surprise, anger, happiness, sadness, fear
Bulut, 2002	English	Simulated	1 actress	Synthesis	Anger, happiness, sadness, neutral
Scherer, 2002	English, German	Natural	100 native speakers	Recognition	Stress
Tato, 2002	German	Elicited	14 native speaker	Synthesis	Anger, boredom, happiness, neutral, sad
Chuang, Wu, 2002	Chinese	Simulated	2 actors (1 Male and 1 female) From male: 558 utterances contained in 137 dialogues From female: 453 sentences in 136 dialogues	Recognition An emotional semantic network proposed to extract the schematic information related to emotion	Anger, surprise, sadness, fear, happiness, antipathy
Yuan, 2002	Chinese	Elicited	9 native speakers	Recognition	Anger, fear, joy, neutral, sadness
Hozjan, 2002	English, Spanish, French	Simulated	One male and one female speaker have been recorded, for English two male and 1 female have been recorded. English interface database contain 8928 sentences, Slovenian contain 6080 sentences, French contain 5600 sentences and Spanish contain 5520 sentences	Synthesis The recorded INTERFACE database is used to develop a multilingual emotion classifier and for multilingual emotion modeling for speech synthesis	Anger, sadness, joy, fear, disgust, surprise, neutral
Rahurkar, Hansen, 2002	English	Ntural	6 soldiers	Recognition	stress
New, 2003	Burmese, Chinese	Simulated	Tweleve speakers Sixty different utterances ten each for each emotion for each speaker was constructed In Burmese six speakers (3 male and 3 female), for Mandarin language (3 male and 3 female) speakers were employed to generate 720 utterances	Recognition Log frequency power coefficients are used for emotion recognition using HMM classifier	Anger, disgust, fear, joy, sadness, surprise
Schroder, Grice, 2003	German	Simulated			NONE
Slaney, McRoberts, 2003	English, German	Natural	12 native speakers (six fathers and six mothers)	Recognition A multidimensional Gaussian mixture-model discriminator classified adult-directed and infant-directed speech using pitch and broad spectral- shapes measures	Approval, attention, prohibition
Lee, Narayanan, 2003	English	Natural	Unknown	Recognition Call center application	anger, frustration, boredom, neutral, happiness
Yamagishi, 2003	Japanese	Simulated	I male speaker Phonetically balanced 503 sentences of ATR Japanese database	Speech recognition and synthesis An approach to realizing various emotional expressions and speaking styles in synthetic speech using HMM based synthesis	Joyful, sad
Schuller, 2003	English, German	Natural and Simulated	5 speakers Total 5250 samples taken for analysis	Recognition Two different methods propagated for various feature analysis and comparison between two classifies such as GMM and HMM	Anger, disgust, fear, surprise, joy, neutral, sadness
Hozjan, Kacic, 2003	English, Spanish, French	Simulated	Total 9 speakers for each language Total 23,000 sentences were recorded. For English language two male and one female speakers were recorded and for Slovenian, Spanish, French language one male and one female speaker were recorded	Recognition Analysis of various acoustic and large set of statistical features	Anger, sadness, joy, fear, disgust, surprise, neutral
Lida, 2003	Japanese	Simulated	Two native speakers (one male and one female)	Synthesis To synthesizing emotional speech by a corpus based concatenative speech synthesis system (ATR CHATR) using speech corpora of emotional speech	Anger, joy, sadness
Fernandez, Picard, 2003	English	Natural	Four drivers	Recognition Use of features derived from multi resolution analysis of speech and TEO for classification of driver’s speech under stressed conditions	Stress
Jovičić, 2004	Serbian	Simulated	Six actors (3 female, 3 male) GEES database contains 32 isolated words, 30 short semantically neutral sentences, 30 long semantically neutral sentences and one passage with 79 words in size. Total database contains 2790 recordings and duration of speech around 3 h	Recognition Designing, processing and evaluation of Serbian emotional speech database	Neutral, anger, happiness, sadness, fear
Schuller, 2004	German, English	Simulated	German and English sentences of 13 speakers, one female were assembled German database contains 2829 emotional recorded samples for training and evaluation in the prosodic and for linguistic analysis English database contains 700 selected utterances in automotive infotainment speech interaction dialogs recorded for the evaluation of the fusion	Recognition Combination of acoustic features and language information for a most robust automatic recognition of a speakers emotion	Anger, disgust, fear, joy, neutral, sadness, surprise
Batliner, 2004	German, English	Elicited	51 children	Recognition Recorded at the university of Maribor, in German and English	Anger, Boredom, joy, surprise
Yildirim, 2004	English	Simulated	One actress 112 utterances per emotion are recorded	Recognition Main aim was how speech is modulated when speakers emotion changes to a certain emotional state. Speech prosody, vowel articulation and spectral energy distribution are to analyze 4 emotions	Sadness, anger, happiness, neutral
Nordstrand, 2004	Swedish	Simulated Multimodal corpus	One native speaker	Synthesis Variations in articulatory parameters are used for recording of Swedish vowels in two emotions	Happiness, neutral
Caldognetto, 2004	Italian	Simulated	Single native speaker	Synthesis Here analysis on the interaction between the articulatory lip targets of the Italian vowel and consonants defined by phonetic-phonological rules and labil configurations peculiar to each emotion	Anger, disgust, fear, joy, sadness, surprise
Jiang, Cai, 2004	Chinese	Simulated	Single amateur actress 200 Chinese utterances for each emotion	Recognition Combination of statistic features and temporal features	Anger, fear, happiness, sadness, surprise, neutral
Ververidis, 2004	Danish	Simulated	Four actors (two male and two female) Danish emotional speech database Total amount of data used in the experiment was 500 speech segments (with no silence interruptions)	Recognition Feature analysis and classification	Anger, happiness, sadness, surprise, neutral
Jiang, 2005	Chinese	Natural	One female speaker Total 216 sad sentences, 143 happy sentences and 10 sentences per each emotion	Synthesis Analysis and modeling the emotional prosody features	Sadness, happiness
Cichosz, Slot, 2005	Polish	Simulated	Four actors and four actresses Total 240 utterances uttered by four actors and four actresses	Recognition To determine a set of low dimensional feature spaces that provides high recognition rates	Anger, fear, sadness, boredom, joy, neutral
Lin, Wei, 2005	Danish	Simulated	Four actors (two male and two female) familiar with radio theatre	Recognition Gender dependent and gender independent speech emotion recognition	Anger, happiness, sadness, surprise, neutral
Luengo, 2005	Basque	Simulated	One actress Total 97 recordings for each emotion were done Database contains numbers, isolated words and sentences of different length	Emotion identification Analysis of prosodic features and spectral features with classifiers GMM and SVM for emotion identification	Anger, fear, surprise, disgust, joy, sadness
Lee, Narayanan, 2005	English	Natural	Customers and call attendants	Recognition Call center conversations are recorded	Negative, positive
Pao, 2005	Chinese	Simulated	Eighteen male and sixteen female uttered 20 different utterances. Total 3400 sentences were recorded	Recognition Evaluation of Mandarin speech using weighted D-KNN classification	Anger, happiness, sadness, boredom, neutral
Batliner, 2006	English	Elicited	51 school children (21 male and 30 Female)	Recognition Children are asked to spontaneously react with Sony AIBO pet robot. Around 9.5 h of effective emotional expressions of children were recorded	NONE
Wu, 2006	Chinese	Simulated	Non –broadcasting speakers Total 25 male, 25 female speakers were involved in the recording process	Recognition Study on GMM-UBM based speaker verification system on emotional speech	Anger, fear, happiness, sadness, neutral
Grimm, 2006	English	Simulated	EMA (Electromagnetic articulography) Database contains 680 emotional speech utterances, generated by one female professional and two non-professional (one male and one female) speakers. Female speaker produce 10 sentences and male speaker produce 14 sentences each for 4 different emotions	Recognition Feature based categorical classification and primitives-based dynamic emotion estimation	Happy, angry, sad, neutral
Morrison, 2007	Chinese, Burmese	Natural and simulated	1. Natural database contains 11 speakers with 388 numbers of utterances for two emotion classes 2. ESMBS database contains 12 emotional speeches of Mandarin and Burmese speakers with 720 utterances for six emotions. Six Mandarin and six Burmese speakers were used. 10 different sentences uttered by the speakers	Recognition Call center applications	Anger, neutral, happiness, sadness, disgust, fear, surprise
Kandali, 2008a	Assamese	Simulated	MESDNEI (multilingual emotional speech database of North East India) database contains short sentences of six full blown basic emotions with neutral Total 140 simulated utterances per speaker was collected for 5 native language of Assam. Specifically students and faculty members from educational institutions were chosen for the recording. 30 subjects (3 male and 3 female per language) were chosen for recording	Recognition Vocal emotion recognition	Anger, disgust, fear, happiness, sadness, surprise, neutral
Grimm, 2008	German	Natural	104 Native speakers (44 male and 60 female)	Recognition 12 h of audio visual recording is done using TV talk show Vera am Mittang in German. Emotion annotation is done based on activation, valence, and dominance dimensions	Activation, Valence, Dominance
Koolagudi, 2009	Telugu	Simulated	The database contains 10 professional artists (5 male and 5 female) from All India Radio (AIR) Vijaywada. Total number of utterances recorded in the database was 12,000 (15 sentences, 8 emotions, 10 artists and 10 sessions). Each emotion contains 1500 utterances	Recognition Design, acquisition, post processing and evaluation of IITKGP-SESC database	Anger, disgust, fear, happy, Compassionate, neutral, sarcastic, surprise
Mohanty, Swain, 2010	Oriya	Elicited	Database contains 35 speakers (Male 23 and Female 12), reading text fragments taken from various Oriya drama scripts	Recognition Creation of Odiya database and emotion recognition from Odiya speech	Anger, sadness, astonish, fear, happiness, neutral
Rao, Koolagudi, 2011	Hindi	Natural and simulated	1. Hindi dialect speech corpus used for dialect identification. It contains 5 females and 5 males, sentences uttered based on their past memories 2. IITKGP-SEHSC corpus used for the speech emotion recognition. It contains 10 professional artists from All India Radio Varanasi, India. Total 12,000 utterances recorded and each emotion has 1500 sentences	Recognition and Identification Dialect identification, emotion recognition and feature analysis	Anger, disgust, fear, happy, neutral, sadness, surprise, sarcastic
Koolagudi, 2012	Hindi	Semi natural	Utterances taken for the database were the recording dialogues delivered by Hindi film actor and actress	Recognition Proposed a Semi natural database (Graphic Era University semi natural speech emotion corpus) for emotion recognition	Sad, anger, happy, neutral
Caballero-Morales, 2013	Spanish	Simulated	Total 6 speakers from the local cultural center of the city of Huajuapan de Leon in Oaxaca, Mexico took part in the recording process. The database contained total 40 utterances and 233 words (vocabulary of 140 unique words)	Recognition Acoustic modeling of emotion-specific vowels	Anger, happiness, neutral, sadness
Quiros-Ramirez, 2014	NONE, Japanese	Natural	Total 57 participants were involved for the recording process, 30 (12 females and 18 males) participants from Latin-American and 27 (10 females and 17 males) from Japan	Recognition Spontaneous cross-cultural emotion database	Negative, positive
Esmaileyan, Marvi, 2014	Persian	Simulated	Persian Drama Radio Emotional Corpus (PDREC) contains emotional utterances taken from radio programs. Total 748 utterances were recorded by 33 (15 females and 18 males) native speakers of Persian language	Recognition Design of database for Automatic Persian speech emotion recognition	Anger, boredom, disgust, fear, neutral, sadness, surprise, happiness
Ooi, 2014	German, English, Chinese, urdu, Punjabi, Persian, Italian	Simulated	1. EMO-DB (Berlin emotional database) contains 10 speakers (5 male and 5 female), 10 sentences were chosen for recording, total 840 recorded utterances were used. 2. eNTERFACE’05 audio-visual emotion database contains 1170 utterances with 42 subjects (34 male and 8 female) chosen from different nations. 3. RML (audio-visual emotion database) contains 720 videos from 8 subjects.	Recognition A new architecture of intelligent audio emotion recognition is introduced and analysis of different prosodic and spectral features was done.	Anger, boredom, disgust, fear, happiness, neutral, sadness, Happy, angry, disgust, sad, surprise, fear, Anger, disgust, fear, happiness, surprise, sadness, neutral
Mencattini, 2014	Italian	Simulated	EMOVO Italian speech corpus. It contains 588 recordings: 14 Italian sentences by 6 professional actors (3 male and 3 female)	Recognition PLS regression model was introduced, new speech features related to speech amplitude modulation parameters was discussed	Disgust, joy, fear, anger, sadness, surprise, neutral
Kadiri, 2015	Telugu, German	Semi natural and simulated	1. Students (two females and five males) were involved in the recording process and the utterances recorded based on past memories. Total 200 utterances recorded for experiment (IIIT-H Telugu emotion database) 2. EMO-DB Berlin emotion database contains 10 professional native German actors (5 males and 5 females) were asked to speak 10 sentences in different emotions. Total 535 utterances were recorded. 339 utterances were taken for final experiment	Recognition Excitation source feature analysis for speech emotion recognition	Anger, happy, neutral, sad
Song, 2016	German, English	Simulated	Berlindataset: emotional utterances recorded by ten actors (5 males and 5 females) in German language. Total 494 utterances were used for experiment eNTERFACE (Audio-visual database) 42 speakers were allotted for recording (34 males and 8 females).Total 1170 video samples were collected	Recognition A novel transfer non-negative matrix factorization (TNMF) method is presented for cross-corpus Speech emotion recognition	Anger, boredom, disgust, fear, happiness, sadness, neutral, Anger, disgust, fear, happiness, sadness, surprise
Brester, 2016	German, English, Japanese	Simulated and natural	Four emotional databases 1. EMO-DB (GERMAN database) recorded at the Technical University of Berlin. It consists of labeled emotional German utterances spoken by 10 actors 2. SAVEE (Surrey Audio-Visual Expressed Emotion) corpus in (English). It contains four native English male speakers 3. LEGO emotion database (English). It comprises of non-acted American English utterances extracted from an automated bus information system of the Carnegie Mellon University at Pittsburg USA 4. UUDB (The Utsunomiya University Spoken Dialouge Database for paralinguistic information studies) (Japanese) consists of spontaneous human –human speech	Recognition Evolutionary feature selection technique based on the two criterion optimization model	Neutral, anger, fear, joy, sadness, boredom, disgust, Anger, disgust, fear, happiness, sadness, surprise, neutral, Angry, neutral
Pravena, Govind, 2017	Tamil, Malayalam, English	Simulated	10 speakers Emotionally biased utterances	Recognition Development of a simulated emotion database for excitation source analysis	Anger, happy, sad