[{"metadata": {"swain_2018": {"reference": "Engberg, Hansen, 1996", "language": "Danish", "actedness": "Simulated", "subjects": "Four actors, two of each gender (two isolated words, nine sentences and two passages)", "goal": "Synthesis\u00a0To evaluate how well the emotional state in emotional speech is identified by humans", "emotions": "Anger, sadness, surprise, neutral, happiness"}}, "language": ["dk"], "emotions": ["ANG", "SAD", "SUR", "NEU", "HAP"], "reference": {"_id": "5ea6966348366c0e33e53fd5", "author": [{"family": "Engberg", "given": "I."}, {"family": "Hansen", "given": "A."}], "date": ["1996"], "title": ["Documentation of the Danish emotional speech database\u201d des"], "note": ["Retrieved from"], "url": ["http://cpk.auc.dk/tb/speech/Emotions/."], "type": null}, "identifier": "EngbergHansen1996", "pseudo_identifier": "EngbergHansen1996", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Montero, 1999", "language": "Spanish", "actedness": "Simulated", "subjects": "Two sessions conducted one professional actor, (3 passages, 15 sentences of neutral-content text). 2000 phonemes per emotion are considered for analysis", "goal": "Synthesis\u00a0Pitch, tempo, and stress are used for synthesis", "emotions": "Happiness, sadness, cold anger, surprise"}}, "language": ["es"], "emotions": ["HAP", "SAD", "CAN", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53ff7", "author": [{"family": "Montero", "given": "J.M."}, {"family": "Guti\u00e9rrez-Arriola", "given": "J."}, {"family": "Col\u00e1s", "given": "J."}, {"family": "Enr\u00edquez", "given": "E."}, {"family": "Pardo", "given": "J.M."}], "date": ["1999"], "title": ["Analysis and modeling of emotional speech in Spanish"], "container-title": ["Proceedings of international conference on phonetic sciences"], "pages": ["957\u2013960"], "type": "paper-conference"}, "identifier": "Montero1999", "pseudo_identifier": "Montero1999", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Amir, 2000", "language": "Hebrew", "actedness": "Natural", "subjects": "40 subjects were considered (19 males, 21 females)", "goal": "Physiologic evaluations\u00a0Signal analysis over sliding windows and extracting a representative feature set", "emotions": "Anger, fear, joy, sadness, disgust"}}, "language": ["he"], "emotions": ["ANG", "FER", "JOY", "SAD", "DIS"], "reference": {"_id": "5ea6966348366c0e33e53fc1", "author": [{"family": "Amir", "given": "N."}, {"family": "Ron", "given": "S."}, {"family": "Laor", "given": "N."}], "date": ["2000"], "title": ["Analysis of an emotional speech corpus in Hebrew based on objective criteria"], "container-title": ["Proceedings of ISCA workshop speech and emotion"], "location": ["Belfast"], "volume": ["1"], "pages": ["29\u201333"], "type": "paper-conference"}, "identifier": "Amir2000", "pseudo_identifier": "Amir2000", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Pereira, 2000", "language": "English", "actedness": "Simulated", "subjects": "2 actors (40 utterances)", "goal": "Recognition\u00a0Findings of emotions on the three dimensional scales arousal, pleasure and power", "emotions": "Happiness, sadness, hot anger, cold anger, neutral"}}, "language": ["en"], "emotions": ["HAP", "SAD", "HAN", "CAN", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53ffd", "author": [{"family": "Pereira", "given": "C."}], "date": ["2000"], "title": ["Dimensions of emotional meaning in speech"], "container-title": ["Proceedings of ISCA workshop speech and emotion"], "location": ["Belfast"], "volume": ["1"], "pages": ["25\u201328"], "type": "paper-conference"}, "identifier": "Pereira2000", "pseudo_identifier": "Pereira2000", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Schr\u00f6der, 2000", "language": "German", "actedness": "Simulated", "subjects": "6 native speakers (3 male, 3 female)", "goal": "Recognition", "emotions": "Admiration, fear, disgust, elation, boredom, relief, startle, worry, contempt, anger"}}, "language": ["de"], "emotions": ["ADM", "FER", "DIS", "ELA", "BOR", "REL", "SUR", "WOR", "CON", "ANG"], "identifier": "Schr\u00f6der2000", "pseudo_identifier": "Schr\u00f6der2000", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Iriondo, 2000", "language": "Spanish", "actedness": "Simulated", "subjects": "Eight actors (4 male and 4 female), 336 discourses were recorded", "goal": "Synthesis", "emotions": "Desire, disgust, fear, anger, joy, sadness, surprise"}}, "language": ["es"], "emotions": ["DSR", "DIS", "FER", "ANG", "JOY", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53fe0", "author": [{"family": "Iriondo", "given": "I."}, {"family": "Guaus", "given": "R."}, {"family": "Rodriguez", "given": "A."}], "date": ["2000"], "title": ["Validation of an acoustical modeling of emotional expression in Spanish using speech synthesis techniques"], "container-title": ["Proceedings of ISCA workshop speech and emotion"], "location": ["Belfast"], "volume": ["1"], "pages": ["161\u2013166"], "type": "paper-conference"}, "identifier": "Iriondo2000", "pseudo_identifier": "Iriondo2000", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Nogueiras, 2001", "language": "Spanish", "actedness": "Simulated", "subjects": "Two professional actors (one male and one female)", "goal": "Synthesis\u00a0Emotion recognition using RAMES, the UOC\u2019s speech recognition system based on standard speech recognition technology using hidden semi-continous Markov models", "emotions": "Anger, disgust, fear, joy, sadness, surprise, neutral"}}, "language": ["es"], "emotions": ["ANG", "DIS", "FER", "JOY", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53ffa", "author": [{"family": "Nogueiras", "given": "A."}, {"family": "Marino", "given": "J.B."}, {"family": "Moreno", "given": "A."}, {"family": "Bonafonte", "given": "A."}], "date": ["2001"], "title": ["Speech emotion recognition using hidden Markov models"], "container-title": ["Proceedings of European conference on speech communication and technology (Eurospeech\u201901"], "location": ["Denmark"], "type": "paper-conference"}, "identifier": "Nogueiras2001", "pseudo_identifier": "Nogueiras2001", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "New, 2001", "language": "Burmese", "actedness": "Simulated or acted", "subjects": "Two Burmese language speakers, 90 emotional utterances each from two speakers", "goal": "Recognition\u00a0A universal codebook is constructed based on emotions", "emotions": "Anger, dislike, fear, happiness, sadness, surprise"}}, "language": ["my"], "emotions": ["ANG", "DLK", "FER", "HAP", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53ff9", "author": [{"family": "New", "given": "T.L."}, {"family": "Wei", "given": "F.S."}, {"family": "De Silva", "given": "L.C."}], "date": ["2001"], "title": ["Speech based emotion classification"], "container-title": ["Proceedings of the IEEE region 10 international conference on electrical and electronic technology (TENCON"], "location": ["Phuket Island, Singapore"], "volume": ["1"], "pages": ["297\u2013301"], "type": "paper-conference"}, "identifier": "New2001", "pseudo_identifier": "New2001", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Yu, 2001", "language": "Chinese", "actedness": "Simulated", "subjects": "Native TV actors\u00a0721 short utterances per emotion are recorded", "goal": "Recognition", "emotions": "Anger, happiness, neutral, sad"}}, "language": ["zh"], "emotions": ["ANG", "HAP", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e5401c", "author": [{"family": "Yu", "given": "F."}, {"family": "Chang", "given": "E."}, {"family": "Xu", "given": "Y.-Q."}, {"family": "Shum", "given": "H.-Y."}], "date": ["2001"], "title": ["Emotion detection from speech to enrich multimedia content"], "container-title": ["Proceedings of IEEE Pacific-Rim Conference on Multimedia"], "location": ["Beijing"], "volume": ["1"], "pages": ["550\u2013557"], "type": "paper-conference"}, "identifier": "Yu2001", "pseudo_identifier": "Yu2001", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Makarova, Petrushin, 2002", "language": "Russian", "actedness": "Simulated", "subjects": "61 Native speakers (12 male, 49 female), 10 sentences were recorded per emotion, total 3660 utterances", "goal": "Recognition\u00a0This database is a source for linguistic and speech processing research", "emotions": "Neutral, surprise, anger, happiness, sadness, fear"}}, "language": ["ru"], "emotions": ["NEU", "SUR", "ANG", "HAP", "SAD", "FER"], "reference": {"_id": "5ea6966348366c0e33e53ff3", "author": [{"family": "Makarova", "given": "V."}, {"family": "Petrushin", "given": "V.A."}], "date": ["2002"], "title": ["RUSLANA: A database of Russian emotional utterances"], "container-title": ["7th International conference on spoken language processing (ICSLP 02"], "pages": ["2041\u20132044"], "type": "paper-conference"}, "identifier": "MakarovaPetrushin2002", "pseudo_identifier": "MakarovaPetrushin2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Bulut, 2002", "language": "English", "actedness": "Simulated", "subjects": "1 actress", "goal": "Synthesis", "emotions": "Anger, happiness, sadness, neutral"}}, "language": ["en"], "emotions": ["ANG", "HAP", "SAD", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fc9", "author": [{"family": "Bulut", "given": "M."}, {"family": "Narayanan", "given": "S.S."}, {"family": "Syrdal", "given": "A.K."}], "date": ["2002"], "title": ["Expressive speech synthesis using a concatenative synthesizer"], "container-title": ["Proceedings of international conference on spoken language processing (ICSLP\u201902"], "volume": ["2"], "pages": ["1265\u20131268"], "type": "paper-conference"}, "identifier": "Bulut2002", "pseudo_identifier": "Bulut2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Scherer, 2002", "language": "English, German", "actedness": "Natural", "subjects": "100 native speakers", "goal": "Recognition", "emotions": "Stress"}}, "language": ["en", "de"], "emotions": ["STR"], "reference": {"_id": "5ea6966348366c0e33e54005", "author": [{"family": "Scherer", "given": "K.R."}, {"family": "Grandjean", "given": "D."}, {"family": "Johnstone", "given": "T."}, {"family": "Klasmeyer", "given": "G."}, {"family": "Banziger", "given": "T."}], "date": ["2002"], "title": ["Acoustic correlates of task load and stress"], "container-title": ["Proceedings of international conference on spoken language processing (ICSLP\u201902), Colorado"], "volume": ["3"], "pages": ["2017\u20132020"], "type": "paper-conference"}, "identifier": "Scherer2002", "pseudo_identifier": "Scherer2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Tato, 2002", "language": "German", "actedness": "Elicited", "subjects": "14 native speaker", "goal": "Synthesis", "emotions": "Anger, boredom, happiness, neutral, sad"}}, "language": ["de"], "emotions": ["ANG", "BOR", "HAP", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e5400f", "author": [{"family": "Tato", "given": "R."}, {"family": "Santos", "given": "R."}, {"family": "Kompe", "given": "R."}, {"family": "Pardo", "given": "J.M."}], "date": ["2002"], "title": ["Emotional space improves emotion recognition"], "container-title": ["Proceedings of international conference on spoken language processing (ICSLP\u201902), Colorado"], "volume": ["3"], "pages": ["2029\u20132032"], "type": "paper-conference"}, "identifier": "Tato2002", "pseudo_identifier": "Tato2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Chuang, Wu, 2002", "language": "Chinese", "actedness": "Simulated", "subjects": "2 actors (1 Male and 1 female)\u00a0From male: 558 utterances contained in 137 dialogues\u00a0From female: 453 sentences in 136 dialogues", "goal": "Recognition\u00a0An emotional semantic network proposed to extract the schematic information related to emotion", "emotions": "Anger, surprise, sadness, fear, happiness, antipathy"}}, "language": ["zh"], "emotions": ["ANG", "SUR", "SAD", "FER", "HAP", "ANT"], "reference": {"_id": "5ea6966348366c0e33e53fce", "author": [{"family": "Chuang", "given": "Z.-J."}, {"family": "Wu", "given": "C.-H."}], "date": ["2002"], "title": ["Emotion recognition from textual input using an emotional semantic network"], "container-title": ["Proceedings of international conference on spoken language processing (ICSLP\u201902"], "volume": ["3"], "pages": ["2033\u20132036"], "type": "paper-conference"}, "identifier": "ChuangWu2002", "pseudo_identifier": "ChuangWu2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Yuan, 2002", "language": "Chinese", "actedness": "Elicited", "subjects": "9 native speakers", "goal": "Recognition", "emotions": "Anger, fear, joy, neutral, sadness"}}, "language": ["zh"], "emotions": ["ANG", "FER", "JOY", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e5401d", "author": [{"family": "Yuan", "given": "J."}, {"family": "Shen", "given": "L."}, {"family": "Chen", "given": "F."}], "date": ["2002"], "title": ["The acoustic realization of anger, fear, joy and sadness in Chinese"], "container-title": ["Proceedings of International Conference on Spoken Language Processing (ICSLP\u201902"], "volume": ["3"], "pages": ["2025\u20132028"], "type": "paper-conference"}, "identifier": "Yuan2002", "pseudo_identifier": "Yuan2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Hozjan, 2002", "language": "English, Spanish, French", "actedness": "Simulated", "subjects": "One male and one female speaker have been recorded, for English two male and 1 female have been recorded. English interface database contain 8928 sentences, Slovenian contain 6080 sentences, French contain 5600 sentences and Spanish contain 5520 sentences", "goal": "Synthesis\u00a0The recorded INTERFACE database is used to develop a multilingual emotion classifier and for multilingual emotion modeling for speech synthesis", "emotions": "Anger, sadness, joy, fear, disgust, surprise, neutral"}}, "language": ["en", "es", "fr"], "emotions": ["ANG", "SAD", "JOY", "FER", "DIS", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fde", "author": [{"family": "Hozjan", "given": "V."}, {"family": "Kacic", "given": "Z."}, {"family": "Moreno", "given": "A."}, {"family": "Bonafonte", "given": "A."}, {"family": "Nogueiras", "given": "A."}], "date": ["2002"], "title": ["Interface databases: Design and collection of a multilingual emotional speech database"], "container-title": ["Proceedings of the 3rd international conference on language (LREC\u201902) Las Palmas de Gran Canaria"], "note": ["Spain, pp. 2019\u20132023."], "type": "paper-conference"}, "identifier": "Hozjan2002", "pseudo_identifier": "Hozjan2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Rahurkar, Hansen, 2002", "language": "English", "actedness": "Ntural", "subjects": "6 soldiers", "goal": "Recognition", "emotions": "stress"}}, "language": ["en"], "emotions": ["STR"], "reference": {"_id": "5ea6966348366c0e33e54001", "author": [{"family": "Rahurkar", "given": "M.A."}, {"family": "Hansen", "given": "J.H."}], "date": ["2002"], "title": ["Frequency band analysis for stress detection using a Teager energy operator based feature"], "container-title": ["Proceedings of International Conference on Spoken Language Processing (ICSLP\u2019"], "volume": ["3"], "pages": ["2021\u20132024"], "issue": ["ue 02"], "type": "article-journal"}, "identifier": "RahurkarHansen2002", "pseudo_identifier": "RahurkarHansen2002", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "New, 2003", "language": "Burmese, Chinese", "actedness": "Simulated", "subjects": "Tweleve speakers\u00a0Sixty different utterances ten each for each emotion for each speaker was constructed\u00a0In Burmese six speakers (3 male and 3 female), for Mandarin language (3 male and 3 female) speakers were employed to generate 720 utterances", "goal": "Recognition\u00a0Log frequency power coefficients are used for emotion recognition using HMM classifier", "emotions": "Anger, disgust, fear, joy, sadness, surprise"}}, "language": ["my", "zh"], "emotions": ["ANG", "DIS", "FER", "JOY", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53f9b", "type_of_reference": "JOUR", "year": "2003", "date": "2003", "title": "Speech emotion recognition using hidden Markov models", "journal_name": "Speech Communication", "volume": "41", "url": "https://doi.org/10.1016/S0167-6393(03)00099-2", "doi": "10.1016/S0167-6393(03)00099-2", "id": "New2003", "author": [{"literal": "New, T. L."}, {"literal": "Wei, F. S."}, {"literal": "Silva, L. C."}]}, "identifier": "DOI:10.1016/S0167-6393(03)00099-2", "pseudo_identifier": "New2003", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Schroder, Grice, 2003", "language": "German", "actedness": "Simulated", "subjects": NaN, "goal": NaN, "emotions": "NONE"}}, "language": ["de"], "emotions": [NaN], "reference": {"_id": "5ea6966348366c0e33e54007", "author": [{"family": "Schroder", "given": "M."}, {"family": "Grice", "given": "M."}], "date": ["2003"], "title": ["Expressing vocal effort in concatenative synthesis"], "container-title": ["Proceedings of international conference on phonetic sciences (ICPhS\u201903"], "location": ["Barcelona"], "pages": ["2589\u20132592"], "type": "paper-conference"}, "identifier": "SchroderGrice2003", "pseudo_identifier": "SchroderGrice2003", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Slaney, McRoberts, 2003", "language": "English, German", "actedness": "Natural", "subjects": "12 native speakers (six fathers and six mothers)", "goal": "Recognition\u00a0A multidimensional Gaussian mixture-model discriminator classified adult-directed and infant-directed speech using pitch and broad spectral- shapes measures", "emotions": "Approval, attention, prohibition"}}, "language": ["en", "de"], "emotions": ["APP", "ATT", "PRO"], "reference": {"_id": "5ea6966348366c0e33e53fb1", "type_of_reference": "JOUR", "year": "2003", "date": "2003", "title": "Babyears: A recognition system for affective vocalizations", "journal_name": "Speech Comunnication", "volume": "39", "url": "https://doi.org/10.1016/S0167-6393(02)00049-3", "doi": "10.1016/S0167-6393(02)00049-3", "id": "Slaney2003", "author": [{"literal": "Slaney, M."}, {"literal": "McRoberts, G."}]}, "identifier": "DOI:10.1016/S0167-6393(02)00049-3", "pseudo_identifier": "SlaneyMcRoberts2003", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Lee, Narayanan, 2003", "language": "English", "actedness": "Natural", "subjects": "Unknown", "goal": "Recognition\u00a0Call center application", "emotions": "anger, frustration, boredom, neutral, happiness"}}, "language": ["en"], "emotions": ["ANG", "FRU", "BOR", "NEU", "HAP"], "reference": {"_id": "5ea6966348366c0e33e53fec", "author": [{"family": "Lee", "given": "C.M."}, {"family": "Narayanan", "given": "S."}], "date": ["2003"], "title": ["Emotion recognition using a data-driven fuzzy inference system"], "container-title": ["European conference on speech and language processing (EUROSPEECH"], "location": ["Geneva, Switzerland"], "pages": ["157\u2013160"], "type": "paper-conference"}, "identifier": "LeeNarayanan2003", "pseudo_identifier": "LeeNarayanan2003", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Yamagishi, 2003", "language": "Japanese", "actedness": "Simulated", "subjects": "I male speaker\u00a0Phonetically balanced 503 sentences of ATR Japanese database", "goal": "Speech recognition and synthesis\u00a0An approach to realizing various emotional expressions and speaking styles in synthetic speech using HMM based synthesis", "emotions": "Joyful, sad"}}, "language": ["ja"], "emotions": ["JOY", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53fb7", "type_of_reference": "BOOK", "year": "2003", "date": "2003", "title": "Emotion recognition using a data-driven fuzzy inference system", "publisher": "Eurospeech", "place_published": "Geneva", "id": "Yamagishi2003", "author": [{"literal": "Yamagishi, J."}, {"literal": "Onishi, K."}, {"literal": "Maskko, T."}, {"literal": "Kobayashi, T."}]}, "identifier": "Yamagishi2003", "pseudo_identifier": "Yamagishi2003", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Schuller, 2003", "language": "English, German", "actedness": "Natural and Simulated", "subjects": "5 speakers\u00a0Total 5250 samples taken for analysis", "goal": "Recognition\u00a0Two different methods propagated for various feature analysis and comparison between two classifies such as GMM and HMM", "emotions": "Anger, disgust, fear, surprise, joy, neutral, sadness"}}, "language": ["en", "de"], "emotions": ["ANG", "DIS", "FER", "SUR", "JOY", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e54009", "author": [{"family": "Schuller", "given": "B."}, {"family": "Rigoll", "given": "G."}, {"family": "Lang", "given": "M."}], "date": ["2003"], "title": ["Hidden Markov model based speech emotion recognition"], "container-title": ["Proceedings of the International conference on multimedia and Expo, ICME"], "type": "paper-conference"}, "identifier": "Schuller2003", "pseudo_identifier": "Schuller2003", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Hozjan, Kacic, 2003", "language": "English, Spanish, French", "actedness": "Simulated", "subjects": "Total 9 speakers for each language\u00a0Total 23,000 sentences were recorded. For English language two male and one female speakers were recorded and for Slovenian, Spanish, French language one male and one female speaker were recorded", "goal": "Recognition\u00a0Analysis of various acoustic and large set of statistical features", "emotions": "Anger, sadness, joy, fear, disgust, surprise, neutral"}}, "language": ["en", "es", "fr"], "emotions": ["ANG", "SAD", "JOY", "FER", "DIS", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f86", "type_of_reference": "BOOK", "year": "2003", "date": "2003", "title": "Improved emotion recognition with large set of stastical features", "publisher": "Eurospecch", "place_published": "Geneva", "id": "Hozjan2003", "author": [{"literal": "Hozjan, V."}, {"literal": "Kacic, Z."}]}, "identifier": "HozjanKacic2003", "pseudo_identifier": "HozjanKacic2003", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Lida, 2003", "language": "Japanese", "actedness": "Simulated", "subjects": "Two native speakers (one male and one female)", "goal": "Synthesis\u00a0To synthesizing emotional speech by a corpus based concatenative speech synthesis system (ATR CHATR) using speech corpora of emotional speech", "emotions": "Anger, joy, sadness"}}, "language": ["ja"], "emotions": ["ANG", "JOY", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53f94", "type_of_reference": "JOUR", "year": "2003", "date": "2003", "title": "A corpus based synthesis system with emotion", "journal_name": "Speech Communication", "volume": "40", "url": "https://doi.org/10.1016/S0167-6393(02)00081-X", "doi": "10.1016/S0167-6393(02)00081-X", "id": "Lida2003", "author": [{"literal": "Lida, A."}, {"literal": "Campbell, N."}, {"literal": "Higuchi, F."}, {"literal": "Yasumura, M."}]}, "identifier": "DOI:10.1016/S0167-6393(02)00081-X", "pseudo_identifier": "Lida2003", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Fernandez, Picard, 2003", "language": "English", "actedness": "Natural", "subjects": "Four drivers", "goal": "Recognition\u00a0Use of features derived from multi resolution analysis of speech and TEO for classification of driver\u2019s speech under stressed conditions", "emotions": "Stress"}}, "language": ["en"], "emotions": ["STR"], "reference": {"_id": "5ea6966348366c0e33e53f82", "type_of_reference": "JOUR", "year": "2003", "date": "2003", "title": "Modeling driver\u2019s speech under stress", "journal_name": "Speech Communication", "volume": "40", "url": "https://doi.org/10.1016/S0167-6393(02)00080-8", "doi": "10.1016/S0167-6393(02)00080-8", "id": "Fernandez2003", "author": [{"literal": "Fernandez, R."}, {"literal": "Picard, R. W."}]}, "identifier": "DOI:10.1016/S0167-6393(02)00080-8", "pseudo_identifier": "FernandezPicard2003", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Jovi\u010di\u0107, 2004", "language": "Serbian", "actedness": "Simulated", "subjects": "Six actors (3 female, 3 male)\u00a0GEES database contains 32 isolated words, 30 short semantically neutral sentences, 30 long semantically neutral sentences and one passage with 79 words in size. Total database contains 2790 recordings and duration of speech around 3\u00a0h", "goal": "Recognition\u00a0Designing, processing and evaluation of Serbian emotional speech database", "emotions": "Neutral, anger, happiness, sadness, fear"}}, "language": ["sr"], "emotions": ["NEU", "ANG", "HAP", "SAD", "FER"], "reference": {"_id": "5ea6966348366c0e33e53fe4", "author": [{"family": "Jovi\u010di\u0107", "given": "S.T."}, {"family": "Ka\u0161i\u0107", "given": "Z."}, {"family": "\u0110or\u0111evi\u0107", "given": "M."}, {"family": "Rajkovi\u0107", "given": "M."}], "date": ["2004"], "title": ["Serbian emotional speech database: Design, processing and evaluation"], "container-title": ["SPECOM 9th conference speech and computer, St"], "location": ["Petersburg, Russia"], "type": "paper-conference"}, "identifier": "Jovi\u010di\u01072004", "pseudo_identifier": "Jovi\u010di\u01072004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Schuller, 2004", "language": "German, English", "actedness": "Simulated", "subjects": "German and English sentences of 13 speakers, one female were assembled\u00a0German database contains 2829 emotional recorded samples for training and evaluation in the prosodic and for linguistic analysis\u00a0English database contains 700 selected utterances in automotive infotainment speech interaction dialogs recorded for the evaluation of the fusion", "goal": "Recognition\u00a0Combination of acoustic features and language information for a most robust automatic recognition of a speakers emotion", "emotions": "Anger, disgust, fear, joy, neutral, sadness, surprise"}}, "language": ["de", "en"], "emotions": ["ANG", "DIS", "FER", "JOY", "NEU", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e5400a", "author": [{"family": "Schuller", "given": "B."}, {"family": "Rigoll", "given": "G."}, {"family": "Lang", "given": "M."}], "date": ["2004"], "title": ["Speech emotion recognition combining acoustic features and linguistis information in a hybrid support vector machine-belief network architecture"], "container-title": ["Proceedings of international conference on acoustics, speech and signal processing (ICASSP\u201904"], "volume": ["1"], "pages": ["557\u2013560"], "type": "paper-conference"}, "identifier": "Schuller2004", "pseudo_identifier": "Schuller2004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Batliner, 2004", "language": "German, English", "actedness": "Elicited", "subjects": "51 children", "goal": "Recognition\u00a0Recorded at the university of Maribor, in German and English", "emotions": "Anger, Boredom, joy, surprise"}}, "language": ["de", "en"], "emotions": ["ANG", "BOR", "JOY", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53fc5", "author": [{"family": "Batliner", "given": "A."}, {"family": "Hacker", "given": "C."}, {"family": "Steidl", "given": "S."}, {"family": "Noth", "given": "E."}, {"family": "D\u2019Arcy", "given": "S."}, {"family": "Russell", "given": "M."}, {"family": "Wong", "given": "M."}], "date": ["2004"], "title": ["You stupid tin box\u2014children interacting with the AIBO robot: A cross-linguistic emotional speech corpus"], "container-title": ["Proceedings of language resources and evaluation (LREC 04"], "location": ["Lisbon"], "type": "paper-conference"}, "identifier": "Batliner2004", "pseudo_identifier": "Batliner2004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Yildirim, 2004", "language": "English", "actedness": "Simulated", "subjects": "One actress\u00a0112 utterances per emotion are recorded", "goal": "Recognition\u00a0Main aim was how speech is modulated when speakers emotion changes to a certain emotional state. Speech prosody, vowel articulation and spectral energy distribution are to analyze 4 emotions", "emotions": "Sadness, anger, happiness, neutral"}}, "language": ["en"], "emotions": ["SAD", "ANG", "HAP", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fee", "author": [{"family": "Lee", "given": "C.M."}, {"family": "Yildirim", "given": "S."}, {"family": "Bulut", "given": "M."}, {"family": "Kazemzadeh", "given": "A."}, {"family": "Busso", "given": "C."}, {"family": "Deng", "given": "Z."}, {"others": true}], "date": ["2004"], "title": ["Emotion recognition based on phoneme classes"], "container-title": ["8th international conference on spokenlanguage processing, INTERSPEECH 2004"], "location": ["Korea"], "type": "paper-conference"}, "identifier": "Yildirim2004", "pseudo_identifier": "Yildirim2004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Nordstrand, 2004", "language": "Swedish", "actedness": "Simulated\u00a0Multimodal corpus", "subjects": "One native speaker", "goal": "Synthesis\u00a0Variations in articulatory parameters are used for recording of Swedish vowels in two emotions", "emotions": "Happiness, neutral"}}, "language": ["sv"], "emotions": ["HAP", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f9d", "type_of_reference": "JOUR", "year": "2004", "date": "2004", "title": "Measurements of ariculatory variation in expressive speech for a set of Swedish vowels", "journal_name": "Speech Communication", "volume": "44", "url": "https://doi.org/10.1016/j.specom.2004.09.003", "doi": "10.1016/j.specom.2004.09.003", "id": "Nordstrand2004", "author": [{"literal": "Nordstrand, L."}, {"literal": "Svanfeld, G."}, {"literal": "Granstrom, B."}, {"literal": "House, D."}]}, "identifier": "DOI:10.1016/j.specom.2004.09.003", "pseudo_identifier": "Nordstrand2004", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Caldognetto, 2004", "language": "Italian", "actedness": "Simulated", "subjects": "Single native speaker", "goal": "Synthesis\u00a0Here analysis on the interaction between the articulatory lip targets of the Italian vowel and consonants defined by phonetic-phonological rules and labil configurations peculiar to each emotion", "emotions": "Anger, disgust, fear, joy, sadness, surprise"}}, "language": ["it"], "emotions": ["ANG", "DIS", "FER", "JOY", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53f7b", "type_of_reference": "JOUR", "year": "2004", "date": "2004", "title": "Modifications of phonetic labial targets in emotive speech: Effects of the co-production of speech and emotions", "journal_name": "Speech Communication", "volume": "44", "url": "https://doi.org/10.1016/j.specom.2004.10.012", "doi": "10.1016/j.specom.2004.10.012", "id": "Caldognetto2004", "author": [{"literal": "Caldognetto, E. M."}, {"literal": "Cosi, P."}, {"literal": "Drioli, C."}, {"literal": "Tisato, G."}, {"literal": "Cavicchio, F."}]}, "identifier": "DOI:10.1016/j.specom.2004.10.012", "pseudo_identifier": "Caldognetto2004", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Jiang, Cai, 2004", "language": "Chinese", "actedness": "Simulated", "subjects": "Single amateur actress\u00a0200 Chinese utterances for each emotion", "goal": "Recognition\u00a0Combination of statistic features and temporal features", "emotions": "Anger, fear, happiness, sadness, surprise, neutral"}}, "language": ["zh"], "emotions": ["ANG", "FER", "HAP", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fe2", "author": [{"family": "Jiang", "given": "D.-N."}, {"family": "Cai", "given": "L.H."}], "date": ["2004"], "title": ["Classifying emotion in Chinese speech by decomposing prosodic features"], "container-title": ["International conference on speech and language processing (ICSLP), Jeju"], "location": ["Korea"], "type": "paper-conference"}, "identifier": "JiangCai2004", "pseudo_identifier": "JiangCai2004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Ververidis, 2004", "language": "Danish", "actedness": "Simulated", "subjects": "Four actors (two male and two female)\u00a0Danish emotional speech database\u00a0Total amount of data used in the experiment was 500 speech segments (with no silence interruptions)", "goal": "Recognition\u00a0Feature analysis and classification", "emotions": "Anger, happiness, sadness, surprise, neutral"}}, "language": ["dk"], "emotions": ["ANG", "HAP", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e54011", "author": [{"family": "Ververidis", "given": "D."}, {"family": "Kotropoulos", "given": "C."}, {"family": "Pitas", "given": "I."}], "date": ["2004"], "title": ["Automatic emotional speech classification"], "container-title": ["Proceedings of international conference on acoustics, speech and signal processing (ICASSP\u201904), Montreal"], "volume": ["1"], "pages": ["593\u2013596"], "type": "paper-conference"}, "identifier": "Ververidis2004", "pseudo_identifier": "Ververidis2004", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Jiang, 2005", "language": "Chinese", "actedness": "Natural", "subjects": "One female speaker\u00a0Total 216 sad sentences, 143 happy sentences and 10 sentences per each emotion", "goal": "Synthesis\u00a0Analysis and modeling the emotional prosody features", "emotions": "Sadness, happiness"}}, "language": ["zh"], "emotions": ["SAD", "HAP"], "reference": {"_id": "5ea6966348366c0e33e53fe3", "author": [{"family": "Jiang", "given": "D.-N."}, {"family": "Zhang", "given": "W."}, {"family": "Shen", "given": "L.-Q."}, {"family": "Cai", "given": "L.-H."}], "date": ["2005"], "title": ["Prosody analysis and modelling for emotional speech synthesis"], "container-title": ["IEEE proceedings of ICASSP 2005"], "pages": ["281\u2013284"], "type": "paper-conference"}, "identifier": "Jiang2005", "pseudo_identifier": "Jiang2005", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Cichosz, Slot, 2005", "language": "Polish", "actedness": "Simulated", "subjects": "Four actors and four actresses\u00a0Total 240 utterances uttered by four actors and four actresses", "goal": "Recognition\u00a0To determine a set of low dimensional feature spaces that provides high recognition rates", "emotions": "Anger, fear, sadness, boredom, joy, neutral"}}, "language": ["pl"], "emotions": ["ANG", "FER", "SAD", "BOR", "JOY", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fcf", "author": [{"family": "Cichosz", "given": "J."}, {"family": "Slot", "given": "K."}], "date": ["2005"], "title": ["Low-dimensional feature space derivation for emotion recognition"], "container-title": ["INTERSPEECH\u201905"], "location": ["Lisbon, Portugal"], "pages": ["477\u2013480"], "type": "chapter"}, "identifier": "CichoszSlot2005", "pseudo_identifier": "CichoszSlot2005", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Lin, Wei, 2005", "language": "Danish", "actedness": "Simulated", "subjects": "Four actors (two male and two female) familiar with radio theatre", "goal": "Recognition\u00a0Gender dependent and gender independent speech emotion recognition", "emotions": "Anger, happiness, sadness, surprise, neutral"}}, "language": ["dk"], "emotions": ["ANG", "HAP", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fef", "author": [{"family": "Lin", "given": "Y.-L."}, {"family": "Wei", "given": "G."}], "date": ["2005"], "title": ["Speech emotion recognition based on HMM and SVM"], "container-title": ["Fourth International conference on machine learning and cybernetics, Guangzhou"], "pages": ["4898\u20134901"], "type": "paper-conference"}, "identifier": "LinWei2005", "pseudo_identifier": "LinWei2005", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Luengo, 2005", "language": "Basque", "actedness": "Simulated", "subjects": "One actress\u00a0Total 97 recordings for each emotion were done\u00a0Database contains numbers, isolated words and sentences of different length", "goal": "Emotion identification\u00a0Analysis of prosodic features and spectral features with classifiers GMM and SVM for emotion identification", "emotions": "Anger, fear, surprise, disgust, joy, sadness"}}, "language": ["eu"], "emotions": ["ANG", "FER", "SUR", "DIS", "JOY", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53ff1", "author": [{"family": "Luengo", "given": "I."}, {"family": "Navas", "given": "E."}, {"family": "Hern\u00e1ez", "given": "I."}, {"family": "S\u00e1nchez", "given": "J."}], "date": ["2005"], "title": ["Automatic emotion recognition using prosodic parameters"], "container-title": ["INTERSPEECH"], "location": ["Lisbon, Portugal"], "pages": ["493\u2013496"], "type": "chapter"}, "identifier": "Luengo2005", "pseudo_identifier": "Luengo2005", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Lee, Narayanan, 2005", "language": "English", "actedness": "Natural", "subjects": "Customers and call attendants", "goal": "Recognition\u00a0Call center conversations are recorded", "emotions": "Negative, positive"}}, "language": ["en"], "emotions": ["NEG", "POS"], "reference": {"_id": "5ea6966348366c0e33e53f92", "type_of_reference": "JOUR", "year": "2005", "date": "2005", "title": "Toward detecting emotions in spoken dialogs", "journal_name": "IEEE Transactions on Speech and Audio Processing", "volume": "13", "url": "https://doi.org/10.1109/TSA.2004.838534", "doi": "10.1109/TSA.2004.838534", "id": "Lee2005", "author": [{"literal": "Lee, C. M."}, {"literal": "Narayanan, S. S."}]}, "identifier": "DOI:10.1109/TSA.2004.838534", "pseudo_identifier": "LeeNarayanan2005", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Pao, 2005", "language": "Chinese", "actedness": "Simulated", "subjects": "Eighteen male and sixteen female uttered 20 different utterances. Total 3400 sentences were recorded", "goal": "Recognition\u00a0Evaluation of Mandarin speech using weighted D-KNN classification", "emotions": "Anger, happiness, sadness, boredom, neutral"}}, "language": ["zh"], "emotions": ["ANG", "HAP", "SAD", "BOR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53ffb", "author": [{"family": "Pao", "given": "T.-L."}, {"family": "Chen", "given": "Y.-T."}, {"family": "Yeh", "given": "J.-H."}, {"family": "Liao", "given": "W.-Y."}], "date": ["2005"], "title": ["Combining acoustic features for improved emotion recognition in Mandarin speech"], "container-title": ["International conference on affective computing and intelligent interaction"], "pages": ["279\u2013285"], "type": "paper-conference"}, "identifier": "Pao2005", "pseudo_identifier": "Pao2005", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Batliner, 2006", "language": "English", "actedness": "Elicited", "subjects": "51 school children (21 male and 30 Female)", "goal": "Recognition\u00a0Children are asked to spontaneously react with Sony AIBO pet robot. Around 9.5\u00a0h of effective emotional expressions of children were recorded", "emotions": "NONE"}}, "language": ["en"], "emotions": [NaN], "reference": {"_id": "5ea6966348366c0e33e53fc4", "author": [{"family": "Batliner", "given": "A."}, {"family": "Biersack", "given": "S."}, {"family": "Steidl", "given": "S."}], "date": ["2006"], "title": ["The prosody of pet robot directed speech: Evidence from children"], "container-title": ["Speech prosody, Dresden"], "pages": ["1\u20134"], "type": "chapter"}, "identifier": "Batliner2006", "pseudo_identifier": "Batliner2006", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Wu, 2006", "language": "Chinese", "actedness": "Simulated", "subjects": "Non \u2013broadcasting speakers\u00a0Total 25 male, 25 female speakers were involved in the recording process", "goal": "Recognition\u00a0Study on GMM-UBM based speaker verification system on emotional speech", "emotions": "Anger, fear, happiness, sadness, neutral"}}, "language": ["zh"], "emotions": ["ANG", "FER", "HAP", "SAD", "NEU"], "reference": {"_id": "5ea6966348366c0e33e54017", "author": [{"family": "Wu", "given": "T."}, {"family": "Yang", "given": "Y."}, {"family": "Wu", "given": "Z."}, {"family": "Li", "given": "D."}], "date": ["2006"], "title": ["MASC: a speech corpus in mandarin for emotion analysis and affective speaker recognition"], "container-title": ["Speaker and language recognition workshop"], "type": "chapter"}, "identifier": "Wu2006", "pseudo_identifier": "Wu2006", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Grimm, 2006", "language": "English", "actedness": "Simulated", "subjects": "EMA (Electromagnetic articulography) Database contains 680 emotional speech utterances, generated by one female professional and two non-professional (one male and one female) speakers. Female speaker produce 10 sentences and male speaker produce 14 sentences each for 4 different emotions", "goal": "Recognition\u00a0Feature based categorical classification and primitives-based dynamic emotion estimation", "emotions": "Happy, angry, sad, neutral"}}, "language": ["en"], "emotions": ["HAP", "ANG", "SAD", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fdb", "author": [{"family": "Grimm", "given": "M."}, {"family": "Mower", "given": "E."}, {"family": "Kroschel", "given": "K."}, {"family": "Narayanan", "given": "S."}], "date": ["2006"], "title": ["Combining categorical and primitives-based emotion recognition"], "container-title": ["14th European signal processing conference (EUSIPCO 2006"], "location": ["Florence, Italy"], "type": "paper-conference"}, "identifier": "Grimm2006", "pseudo_identifier": "Grimm2006", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Morrison, 2007", "language": "Chinese, Burmese", "actedness": "Natural and simulated", "subjects": "1. Natural database contains 11 speakers with 388 numbers of utterances for two emotion classes\u00a02. ESMBS database contains 12 emotional speeches of Mandarin and Burmese speakers with 720 utterances for six emotions. Six Mandarin and six Burmese speakers were used. 10 different sentences uttered by the speakers", "goal": "Recognition\u00a0Call center applications", "emotions": "Anger, neutral, happiness, sadness, disgust, fear, surprise"}}, "language": ["zh", "my"], "emotions": ["ANG", "NEU", "HAP", "SAD", "DIS", "FER", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53f98", "type_of_reference": "JOUR", "year": "2007", "date": "2007", "title": "Ensemble methods for spoken emotion recognition in call-centres", "journal_name": "Speech Communication", "volume": "49", "url": "https://doi.org/10.1016/j.specom.2006.11.004", "doi": "10.1016/j.specom.2006.11.004", "id": "Morrison2007", "author": [{"literal": "Morrison, D."}, {"literal": "Wang, R."}, {"literal": "Silva, L. C."}]}, "identifier": "DOI:10.1016/j.specom.2006.11.004", "pseudo_identifier": "Morrison2007", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Kandali, 2008a", "language": "Assamese", "actedness": "Simulated", "subjects": "MESDNEI (multilingual emotional speech database of North East India) database contains short sentences of six full blown basic emotions with neutral\u00a0Total 140 simulated utterances per speaker was collected for 5 native language of Assam. Specifically students and faculty members from educational institutions were chosen for the recording. 30 subjects (3 male and 3 female per language) were chosen for recording", "goal": "Recognition\u00a0Vocal emotion recognition", "emotions": "Anger, disgust, fear, happiness, sadness, surprise, neutral"}}, "language": ["as"], "emotions": ["ANG", "DIS", "FER", "HAP", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53fe6", "author": [{"family": "Kandali", "given": "A.B."}, {"family": "Routray", "given": "A."}, {"family": "Basu", "given": "T.K."}], "date": ["2008"], "title": ["Emotion recognition from Assamese speeches using MFCC features and GMM classifier"], "container-title": ["Proceedings of IEEE region 10 conference on TENCHON"], "type": "paper-conference"}, "identifier": "Kandali2008", "pseudo_identifier": "Kandali2008", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Grimm, 2008", "language": "German", "actedness": "Natural", "subjects": "104 Native speakers (44 male and 60 female)", "goal": "Recognition\u00a012\u00a0h of audio visual recording is done using TV talk show Vera am Mittang in German. Emotion annotation is done based on activation, valence, and dominance dimensions", "emotions": "Activation, Valence, Dominance"}}, "language": ["de"], "emotions": ["ACT", "VAL", "DOM"], "reference": {"_id": "5ea6966348366c0e33e53fda", "author": [{"family": "Grimm", "given": "M."}, {"family": "Kroschel", "given": "K."}, {"family": "Narayanan", "given": "S."}], "date": ["2008"], "title": ["The Vera Ammittag German audio-visual emotional speech database"], "container-title": ["International conference on multimedia and expo"], "pages": ["865\u2013868"], "type": "paper-conference"}, "identifier": "Grimm2008", "pseudo_identifier": "Grimm2008", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Koolagudi, 2009", "language": "Telugu", "actedness": "Simulated", "subjects": "The database contains 10 professional artists (5 male and 5 female) from All India Radio (AIR) Vijaywada. Total number of utterances recorded in the database was 12,000 (15 sentences, 8 emotions, 10 artists and 10 sessions). Each emotion contains 1500 utterances", "goal": "Recognition\u00a0Design, acquisition, post processing and evaluation of IITKGP-SESC database", "emotions": "Anger, disgust, fear, happy, Compassionate, neutral, sarcastic, surprise"}}, "language": ["te"], "emotions": ["ANG", "DIS", "FER", "HAP", "COM", "NEU", "SAR", "SUR"], "reference": {"_id": "5ea6966348366c0e33e53f8c", "type_of_reference": "BOOK", "year": "2009", "date": "2009", "title": "IITKGP-SESC: Speech database for emotion analysis. Communications in computer and information science, LNCS", "publisher": "Springer", "place_published": "Berlin", "id": "Koolagudi2009", "author": [{"literal": "Koolagudi, S. G."}, {"literal": "Maity, S."}, {"literal": "Kumar, V. A."}, {"literal": "Chakrabati, S."}, {"literal": "Rao, K. S."}]}, "identifier": "Koolagudi2009", "pseudo_identifier": "Koolagudi2009", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Mohanty, Swain, 2010", "language": "Oriya", "actedness": "Elicited", "subjects": "Database contains 35 speakers (Male 23 and Female 12), reading text fragments taken from various Oriya drama scripts", "goal": "Recognition\u00a0Creation of Odiya database and emotion recognition from Odiya speech", "emotions": "Anger, sadness, astonish, fear, happiness, neutral"}}, "language": ["or"], "emotions": ["ANG", "SAD", "AST", "FER", "HAP", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53ff6", "author": [{"family": "Mohanty", "given": "S."}, {"family": "Swain", "given": "B.K."}], "date": ["2010"], "title": ["Emotion recognition using fuzzy K-means from Oriya speech"], "container-title": ["International Conference [ACCTA-2010"], "note": ["on Special Issue of IJCCT, Vol. 1 Issue 2\u20134."], "type": "paper-conference"}, "identifier": "MohantySwain2010", "pseudo_identifier": "MohantySwain2010", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Rao, Koolagudi, 2011", "language": "Hindi", "actedness": "Natural and simulated", "subjects": "1. Hindi dialect speech corpus used for dialect identification. It contains 5 females and 5 males, sentences uttered based on their past memories\u00a02. IITKGP-SEHSC corpus used for the speech emotion recognition. It contains 10 professional artists from All India Radio Varanasi, India. Total 12,000 utterances recorded and each emotion has 1500 sentences", "goal": "Recognition and Identification\u00a0Dialect identification, emotion recognition and feature analysis", "emotions": "Anger, disgust, fear, happy, neutral, sadness, surprise, sarcastic"}}, "language": ["hi"], "emotions": ["ANG", "DIS", "FER", "HAP", "NEU", "SAD", "SUR", "SAR"], "reference": {"_id": "5ea6966348366c0e33e53fa6", "type_of_reference": "JOUR", "year": "2011", "date": "2011", "title": "Identification of Hindi dialects and emotions using spectral and prosodic features of speech", "journal_name": "Systemics, Cybernetics, and Informatics", "volume": "9", "id": "Rao2011", "author": [{"literal": "Rao, K. S."}, {"literal": "Koolagudi, S. G."}]}, "identifier": "RaoKoolagudi2011", "pseudo_identifier": "RaoKoolagudi2011", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Koolagudi, 2012", "language": "Hindi", "actedness": "Semi natural", "subjects": "Utterances taken for the database were the recording dialogues delivered by Hindi film actor and actress", "goal": "Recognition\u00a0Proposed a Semi natural database (Graphic Era University semi natural speech emotion corpus) for emotion recognition", "emotions": "Sad, anger, happy, neutral"}}, "language": ["hi"], "emotions": ["SAD", "ANG", "HAP", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f8a", "type_of_reference": "JOUR", "year": "2012", "date": "2012", "title": "Recognition of emotions from speech using excitation source features", "journal_name": "Procedia Engineering", "volume": "38", "url": "https://doi.org/10.1016/j.proeng.2012.06.394", "doi": "10.1016/j.proeng.2012.06.394", "id": "Koolagudi2012", "author": [{"literal": "Koolagudi, S. G."}, {"literal": "Devliyal, S."}, {"literal": "Chawla, B."}, {"literal": "Barthwal, A."}, {"literal": "Rao, K. S."}]}, "identifier": "DOI:10.1016/j.proeng.2012.06.394", "pseudo_identifier": "Koolagudi2012", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Caballero-Morales, 2013", "language": "Spanish", "actedness": "Simulated", "subjects": "Total 6 speakers from the local cultural center of the city of Huajuapan de Leon in Oaxaca, Mexico took part in the recording process. The database contained total 40 utterances and 233 words (vocabulary of 140 unique words)", "goal": "Recognition\u00a0Acoustic modeling of emotion-specific vowels", "emotions": "Anger, happiness, neutral, sadness"}}, "language": ["es"], "emotions": ["ANG", "HAP", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53fcc", "author": [{"family": "Caballero-Morales", "given": "S.O."}], "date": ["2013", "2013, 1\u201313"], "title": ["Recognition of emotions in Mexican Spanish speech: An approach based on acoustic modelling of emotion-specific vowels"], "type": "article-journal", "container-title": ["The Scientific World Journal"]}, "identifier": "10.1155/2013/162093", "pseudo_identifier": "Caballero-Morales2013", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Quiros-Ramirez, 2014", "language": "NONE, Japanese", "actedness": "Natural", "subjects": "Total 57 participants were involved for the recording process, 30 (12 females and 18 males) participants from Latin-American and 27 (10 females and 17 males) from Japan", "goal": "Recognition\u00a0Spontaneous cross-cultural emotion database", "emotions": "Negative, positive"}}, "language": [NaN, "ja"], "emotions": ["NEG", "POS"], "reference": {"_id": "5ea6966348366c0e33e54000", "author": [{"family": "Quiros-Ramirez", "given": "M.A."}, {"family": "Polikovsky", "given": "S."}, {"family": "Kameda", "given": "Y."}, {"family": "Onisawa", "given": "T."}], "date": ["2014"], "title": ["A spontaneous cross-cultural emotion database: Latin-America vs. Japan"], "container-title": ["International conference on Kansei Engineering and emotion research"], "pages": ["1127\u20131134"], "type": "paper-conference"}, "identifier": "Quiros-Ramirez2014", "pseudo_identifier": "Quiros-Ramirez2014", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Esmaileyan, Marvi, 2014", "language": "Persian", "actedness": "Simulated", "subjects": "Persian Drama Radio Emotional Corpus (PDREC) contains emotional utterances taken from radio programs. Total 748 utterances were recorded by 33 (15 females and 18 males) native speakers of Persian language", "goal": "Recognition\u00a0Design of database for Automatic Persian speech emotion recognition", "emotions": "Anger, boredom, disgust, fear, neutral, sadness, surprise, happiness"}}, "language": ["fa"], "emotions": ["ANG", "BOR", "DIS", "FER", "NEU", "SAD", "SUR", "HAP"], "reference": {"_id": "5ea6966348366c0e33e53f81", "type_of_reference": "JOUR", "year": "2014", "date": "2014", "title": "A database for automatic Persian speech emotion recognition: Collection, processing and evaluation", "journal_name": "IJE Transactions A: Bascis", "volume": "27", "id": "Esmaileyan2014", "author": [{"literal": "Esmaileyan, Z."}, {"literal": "Marvi, H."}]}, "identifier": "EsmaileyanMarvi2014", "pseudo_identifier": "EsmaileyanMarvi2014", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Ooi, 2014", "language": "German, English, Chinese, urdu, Punjabi, Persian, Italian", "actedness": "Simulated", "subjects": "1. EMO-DB (Berlin emotional database) contains 10 speakers (5 male and 5 female), 10 sentences were chosen for recording, total 840 recorded utterances were used.\u00a02. eNTERFACE\u201905 audio-visual emotion database contains 1170 utterances with 42 subjects (34 male and 8 female) chosen from different nations.\u00a03. RML (audio-visual emotion database) contains 720 videos from 8 subjects.", "goal": "Recognition\u00a0A new architecture of intelligent audio emotion recognition is introduced and analysis of different prosodic and spectral features was done.", "emotions": "Anger, boredom, disgust, fear, happiness, neutral, sadness, Happy, angry, disgust, sad, surprise, fear, Anger, disgust, fear, happiness, surprise, sadness, neutral"}}, "language": ["de", "en", "zh", "ur", "pa", "fa", "it"], "emotions": ["ANG", "BOR", "DIS", "FER", "HAP", "NEU", "SAD", "HAP", "ANG", "DIS", "SAD", "SUR", "FER", "ANG", "DIS", "FER", "HAP", "SUR", "SAD", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f9e", "type_of_reference": "JOUR", "year": "2014", "date": "2014", "title": "A new approach of audio emotion recognition", "journal_name": "Experts Systems with Applications", "volume": "41", "url": "https://doi.org/10.1016/j.eswa.2014.03.026", "doi": "10.1016/j.eswa.2014.03.026", "id": "Ooi2014", "author": [{"literal": "Ooi, C. S."}, {"literal": "Seng, K. P."}, {"literal": "Ang, L. -. M."}, {"literal": "Chew, L. W."}]}, "identifier": "DOI:10.1016/j.eswa.2014.03.026", "pseudo_identifier": "Ooi2014", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Mencattini, 2014", "language": "Italian", "actedness": "Simulated", "subjects": "EMOVO Italian speech corpus. It contains 588 recordings: 14 Italian sentences by 6 professional actors (3 male and 3 female)", "goal": "Recognition\u00a0PLS regression model was introduced, new speech features related to speech amplitude modulation parameters was discussed", "emotions": "Disgust, joy, fear, anger, sadness, surprise, neutral"}}, "language": ["it"], "emotions": ["DIS", "JOY", "FER", "ANG", "SAD", "SUR", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f97", "type_of_reference": "JOUR", "year": "2014", "date": "2014", "title": "Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure", "journal_name": "Knowledge-Based Systems", "volume": "63", "url": "https://doi.org/10.1016/j.knosys.2014.03.019", "doi": "10.1016/j.knosys.2014.03.019", "id": "Mencattini2014", "author": [{"literal": "Mencattini, A."}, {"literal": "Martinelli, E."}, {"literal": "Costantini, G."}, {"literal": "Todisco, M."}, {"literal": "Basile, B."}, {"literal": "Bozzali, M."}, {"literal": "Natale, C."}]}, "identifier": "DOI:10.1016/j.knosys.2014.03.019", "pseudo_identifier": "Mencattini2014", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Kadiri, 2015", "language": "Telugu, German", "actedness": "Semi natural and simulated", "subjects": "1. Students (two females and five males) were involved in the recording process and the utterances recorded based on past memories. Total 200 utterances recorded for experiment (IIIT-H Telugu emotion database)\u00a02. EMO-DB Berlin emotion database contains 10 professional native German actors (5 males and 5 females) were asked to speak 10 sentences in different emotions. Total 535 utterances were recorded. 339 utterances were taken for final experiment", "goal": "Recognition\u00a0Excitation source feature analysis for speech emotion recognition", "emotions": "Anger, happy, neutral, sad"}}, "language": ["te", "de"], "emotions": ["ANG", "HAP", "NEU", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53fe5", "author": [{"family": "Kadiri", "given": "S.R."}, {"family": "Gangamohan", "given": "P."}, {"family": "Gangashetty", "given": "S.V."}, {"family": "Yegnanarayana", "given": "B."}], "date": ["2015", "1324"], "title": ["Analysis of excitation source features of speech for emotion recognition"], "container-title": ["INTERSPEECH 2015"], "location": ["Dresden"], "type": "chapter"}, "identifier": "Kadiri2015", "pseudo_identifier": "Kadiri2015", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Song, 2016", "language": "German, English", "actedness": "Simulated", "subjects": "Berlindataset: emotional utterances recorded by ten actors (5 males and 5 females) in German language. Total 494 utterances were used for experiment\u00a0eNTERFACE (Audio-visual database) 42 speakers were allotted for recording (34 males and 8 females).Total 1170 video samples were collected", "goal": "Recognition\u00a0A novel transfer non-negative matrix factorization (TNMF) method is presented for cross-corpus Speech emotion recognition", "emotions": "Anger, boredom, disgust, fear, happiness, sadness, neutral, Anger, disgust, fear, happiness, sadness, surprise"}}, "language": ["de", "en"], "emotions": ["ANG", "BOR", "DIS", "FER", "HAP", "SAD", "NEU", "ANG", "DIS", "FER", "HAP", "SAD", "SUR"], "reference": {"_id": "5ea6966348366c0e33e5400b", "author": [{"family": "Song", "given": "P."}, {"family": "Ou", "given": "S."}, {"family": "Zheng", "given": "W."}, {"family": "Jin", "given": "Y."}, {"family": "Zhao", "given": "L."}], "date": ["2016"], "title": ["Speech emotion recognition using transfer non-negative matrix factorization"], "container-title": ["Proceedings of IEEE international conference ICASSP"], "pages": ["5180\u20135184"], "type": "paper-conference"}, "identifier": "Song2016", "pseudo_identifier": "Song2016", "incomplete": true, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Brester, 2016", "language": "German, English, Japanese", "actedness": "Simulated and natural", "subjects": "Four emotional databases\u00a01. EMO-DB (GERMAN database) recorded at the Technical University of Berlin. It consists of labeled emotional German utterances spoken by 10 actors\u00a02. SAVEE (Surrey Audio-Visual Expressed Emotion) corpus in (English). It contains four native English male speakers\u00a03. LEGO emotion database (English). It comprises of non-acted American English utterances extracted from an automated bus information system of the Carnegie Mellon University at Pittsburg USA\u00a04. UUDB (The Utsunomiya University Spoken Dialouge Database for paralinguistic information studies) (Japanese) consists of spontaneous human \u2013human speech", "goal": "Recognition\u00a0Evolutionary feature selection technique based on the two criterion optimization model", "emotions": "Neutral, anger, fear, joy, sadness, boredom, disgust, Anger, disgust, fear, happiness, sadness, surprise, neutral, Angry, neutral"}}, "language": ["de", "en", "ja"], "emotions": ["NEU", "ANG", "FER", "JOY", "SAD", "BOR", "DIS", "ANG", "DIS", "FER", "HAP", "SAD", "SUR", "NEU", "ANG", "NEU"], "reference": {"_id": "5ea6966348366c0e33e53f79", "type_of_reference": "JOUR", "year": "2016", "date": "2016", "title": "Multi-objective heuristic feature selection for speech-based multilingual emotion recognition", "journal_name": "JAISCR", "volume": "6", "id": "Brester2016", "author": [{"literal": "Brester, C."}, {"literal": "Semenkin, E."}, {"literal": "Sidorov, M."}]}, "identifier": "10.1515/jaiscr-2016-0018", "pseudo_identifier": "Brester2016", "incomplete": false, "source": "swain_2018"}, {"metadata": {"swain_2018": {"reference": "Pravena, Govind, 2017", "language": "Tamil, Malayalam, English", "actedness": "Simulated", "subjects": "10 speakers\u00a0Emotionally biased utterances", "goal": "Recognition\u00a0Development of a simulated emotion database for excitation source analysis", "emotions": "Anger, happy, sad"}}, "language": ["ta", "ml", "en"], "emotions": ["ANG", "HAP", "SAD"], "reference": {"_id": "5ea6966348366c0e33e53fa3", "type_of_reference": "JOUR", "year": "2017", "date": "2017", "title": "Significance of incorporating excitation source parameters for improved emotion recognition from speech and electroglottographic signals", "journal_name": "International Journal of Speech Technology", "volume": "20", "url": "https://doi.org/10.1007/s10772-017-9445-x", "doi": "10.1007/s10772-017-9445-x", "id": "Pravena2017", "author": [{"literal": "Pravena, D."}, {"literal": "Govind, D."}]}, "identifier": "DOI:10.1007/s10772-017-9445-x", "pseudo_identifier": "PravenaGovind2017", "incomplete": false, "source": "swain_2018"}]