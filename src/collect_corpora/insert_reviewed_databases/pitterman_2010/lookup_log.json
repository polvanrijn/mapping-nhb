{"Navas2004a": false, "Saratxaga2006": false, "Nwe2001": false, "YangCampbell2001": false, "Yuan2002": false, "ChuangWu2004": {"names": "Chuang, ZJ; Wu, CH", "full_names": "Chuang, ZJ; Wu, CH", "title": "Emotion recognition using acoustic features and textual content", "journal": "2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "This paper presents an approach to emotion recognition from speech signals and textual content. In the analysis of speech signals, thirty-three acoustic features are extracted from the speech input. After Principle Component Analysis (PCA), 14 principle components are selected for discriminative representation. In this representation each principle component is the combination of the 33 original acoustic features and forms a feature subspace. The Support Vector Machines (SVMs) are adopted to classify the emotional states. In text analysis, all emotional keywords and emotion modification words are manually defined. The emotion intensity levels of emotional keywords and emotion modification words are estimated from a collected emotion corpus. The final emotional state is determined based on the emotion outputs from the acoustic and textual approaches. The experimental result shows that the emotion recognition accuracy of the integrated system is better than each of the two individual approaches.", "affiliation": "Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan", "author_information": "Chuang, ZJ (reprint author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.", "email": NaN, "citation_count_WOS": 6, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2004, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Imaging Science & Photographic Technology; Telecommunications", "category2": "Computer Science; Imaging Science & Photographic Technology; Telecommunications", "DOI": "10.1109/ICME.2004.1394123"}, "Pao2004": false, "Tao2004": false, "Tao2006": false, "Wu2006": {"names": "Wu, W; Zheng, TF; Xu, MX; Bao, HJ", "full_names": "Wu, Wei; Zheng, Thomas Fang; Xu, Ming-Xing; Bao, Huan-Jun", "title": "Study on Speaker Verification on Emotional Speech", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "speaker verification; emotional speech", "keywords_plus": "RECOGNITION; MODELS", "abstract": "Besides background noise, channel effect and speaker's health condition, emotion is another factor which may influence the performance of a speaker verification system. In this paper, the performance of a GMM-UBM based speaker verification system on emotional speech is studied. It is found that speech with various emotions aggravates the verification performance. Two reasons for the performance aggravation are analyzed, they are mismatched emotions between the speaker models and the test utterances, and the articulating styles of certain emotions which create intense intra-speaker vocal variability. In response to the first reason, an emotion-dependent score normalization method is proposed, which is borrowed from the idea of Hnorm.", "affiliation": "[Wu, Wei; Zheng, Thomas Fang; Xu, Ming-Xing; Bao, Huan-Jun] Tsinghua Univ, Ctr Speech Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China", "author_information": "Wu, W (reprint author), Tsinghua Univ, Ctr Speech Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.", "email": "wuwei@cst.cs.tsinghua.edu.cn; fzheng@tsinghua.edu.cn; xumx@tsinghua.edu.cn; baohi@cst.cs.tsinghua.edu.cn", "citation_count_WOS": 9, "wos_usage_count_180": 0, "wos_usage_count_2013": 2, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Zhang2006": {"names": "Zhang, S; Ching, PC; Kong, FR", "full_names": "Zhang, Sheng; Ching, P. C.; Kong, Fanrang", "title": "Automatic Emotion Recognition of Speech Signal in Mandarin", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "Mandarin; automatic emotion recognition; cascade bisection process (CB-process)", "keywords_plus": NaN, "abstract": "Traditionally, a simultaneous recognition process using the same feature set of a spoken utterance is used to classify the emotional state of the speaker in addition to its content. However, an analysis on the classification performance for every pair of emotions shows that different features have distinctive classification abilities for different emotions. Therefore, we propose an efficient emotion recognition process called cascade bisection (CB-process), which carries out emotion recognition by means of several bisecting steps and applies different feature sets for every step. This process is based on the features' different abilities of classifying emotions. Through this, we can fully utilize the information extracted from features and achieve a better recognition performance. Five discrete emotional states, namely, neutral, anger, fear, joy, and sadness are distinguished from the input Mandarin speech. After extracting the acoustic features that contain information on short-time energy (amplitude), signal amplitude, and pitch, we derive the representation feature set for further use in the CB-process, which achieves better emotion recognition as demonstrated seen from the experimental results.", "affiliation": "[Zhang, Sheng; Kong, Fanrang] Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, Hefei 230027, Anhui, Peoples R China; [Ching, P. C.] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China", "author_information": "Zhang, S (reprint author), Univ Sci & Technol China, Dept Precis Machinery & Precis Instrumentat, Hefei 230027, Anhui, Peoples R China.", "email": NaN, "citation_count_WOS": 9, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Campbell2002": false, "Jiang2005": {"names": "Jiang, XQ; Tian, L; Han, M", "full_names": "Jiang, XQ; Tian, L; Han, M", "title": "Separability and recognition of emotion states in multilingual speech", "journal": "2005 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS, VOLS 1 AND 2, PROCEEDINGS: VOL 1: COMMUNICATION THEORY AND SYSTEMS", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "In this paper we analyze, the separability and recognition of emotion states in multilingual speech signals statistically. Prosodic features such as pitch, energy and time parameters are extracted and the separability is discussed based on the statistical results in a two-dimension method. Principle Component Analysis (PCA) is then used to recognize emotion states and achieved satisfying results, in which mean recognition rate is 72.26% and the highest recognition rate is 89%. The results show that language factors don't effect features of prosodic variation of some given emotion obviously and basic emotions can be recognized roughly from speech signals using prosodic parameters.", "affiliation": "Shandong Univ, Sch Informat Sci & Engn, Shandong, Peoples R China", "author_information": "Jiang, XQ (reprint author), Shandong Univ, Sch Informat Sci & Engn, Shandong, Peoples R China.", "email": "xqjiang@sdu.edu.cn; tianlan65@sdu.edu.cn", "citation_count_WOS": 9, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": NaN, "issue": NaN, "category1": "Engineering, Electrical & Electronic; Telecommunications", "category2": "Engineering; Telecommunications", "DOI": NaN}, "Engberg1997": false, "Wilting2006": {"names": "Wilting, J; Krahmer, E; Swerts, M", "full_names": "Wilting, Janneke; Krahmer, Emiel; Swerts, Marc", "title": "Real vs. acted emotional speech", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "Emotional Speech; Audiovisual Speech; Acting", "keywords_plus": "INDUCTION", "abstract": "Even though the use of actors is a popular method for researching the expression of emotion, little is known about the relation between acted and real emotions. To shed some light on this, we set up a novel experiment, based on the Velten mood induction procedure, during which participants have to utter pre-defined sentences with a strong emotional content. In one group of participants, real positive or negative emotions were induced, while another group was instructed to act positive or negative while uttering Velten sentences. Results of a mood questionnaire revealed that participants in the real emotion condition, indeed felt positive or negative, depending on whether they read positive or negative sentences, while participants in the acted emotion condition felt neutral afterwards. In a second, perception experiment, it was found that acted emotions (especially negative ones) were perceived more strongly than the real emotions. This suggests that actors do not feel the acted emotion, and may engage in overacting, which casts doubt on the usefulness of actors as a way to study real emotions.", "affiliation": "[Wilting, Janneke; Krahmer, Emiel; Swerts, Marc] Tilburg Univ, Tilburg, Netherlands", "author_information": "Wilting, J (reprint author), Tilburg Univ, Tilburg, Netherlands.", "email": "E.J.Krahmer@uvt.nl; M.G.J.Swerts@uvt.nl", "citation_count_WOS": 11, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "CowieDouglas-Cowie1996": {"names": "Cowie, R; DouglasCowie, E", "full_names": "Cowie, R; DouglasCowie, E", "title": "Automatic statistical analysis of the signal and prosodic signs of emotion in speech", "journal": "ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "We highlight two broader domains surrounding specific attributions of emotion and the specific features of speech that underlie them, and argue for caution over compartmentalising these broader domains. It seems to be a general rule that variations in what we call the augmented prosodic domain (APD) are emotive - perhaps because they signal departure from a reference point corresponding to a well-controlled, neutral state. Our studies show that various departures from that reference point are reflected in the APD, including central and sensory impairments (schizophrenia and deafness) as well as emotion. Intuitively it seems right to acknowledge that departures from well-controlled neutrality are highly confusable, and it is unclear that phonetics should to try draw those distinctions more sharply than listeners tend to. A system called ASSESS automatically measures properties in the APD, opening the way to explore it in an empirical spirit.", "affiliation": NaN, "author_information": "Cowie, R (reprint author), QUEENS UNIV BELFAST,SCH PSYCHOL,BELFAST BT7 1NN,ANTRIM,NORTH IRELAND.", "email": NaN, "citation_count_WOS": 0, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 1996, "volume": NaN, "issue": NaN, "category1": "Acoustics; Engineering, Electrical & Electronic; Language & Linguistics; Psychology, Experimental", "category2": "Acoustics; Engineering; Linguistics; Psychology", "DOI": NaN}, "Hansen1998": false, "LiZhao1998": false, "Whiteside1998": false, "Cowie1999b": false, "RobsonMackenzie-Beck1999": false, "Greasley2000": {"names": "Greasley, P; Sherrard, C; Waterman, M", "full_names": "Greasley, P; Sherrard, C; Waterman, M", "title": "Emotion in language and speech: Methodological issues in naturalistic approaches", "journal": "LANGUAGE AND SPEECH", "language": "English", "publication_type": "Article", "author_keywords": "context; emotion; judgment; valency; validity", "keywords_plus": "SEMANTIC ASSOCIATIONS; FACIAL EXPRESSIONS; PERCEPTION; STORIES; LEXICON; TONE", "abstract": "Researchers currently seek to improve Validity in speech and language studies by adopting naturalistic procedures In emotion-display research, validity is threatened by standard experimental controls which diminish the naturalism of stimuli and response ranges. We report two experiments comparing the adequacy of naturalistic with standard procedures. Experiment I had 158 judges code 89 samples of naturally-occurring emotional speech with free-choice emotion labels, and later with labels from a standard set. When free-choice labels were similar across judges, they were consistent with standard labels, but showed a range of intensity and contextual relevance. We recommend that future studies include wider options for judges when coding emotions. Experiment 2 compared valency ratings of words when presented in, or out of, context. Standard procedures score lexical valencies using affective dictionaries, disregarding natural contexts. Experiment 2 compared 23 judges' valency ratings of wards presented individually, and later in their original context. Between 30% and 44% of words were rated differently in context (depending on the statistical significance level adopted). We concluded from Experiment 2 that, where small corpora adequately model a domain, the improved accuracy of valency rating achieved by presenting words in their natural context justifies the extra procedures required.", "affiliation": "Univ Leeds, Sch Psychol, Leeds LS2 9JT, W Yorkshire, England", "author_information": "Waterman, M (reprint author), Univ Leeds, Sch Psychol, Leeds LS2 9JT, W Yorkshire, England.", "email": NaN, "citation_count_WOS": 37, "wos_usage_count_180": 1, "wos_usage_count_2013": 11, "journal_abbr": "Lang. Speech", "month_day": "OCT-DEC", "year": 2000, "volume": 43, "issue": NaN, "category1": "Audiology & Speech-Language Pathology; Linguistics; Psychology, Experimental", "category2": "Audiology & Speech-Language Pathology; Linguistics; Psychology", "DOI": "10.1177/00238309000430040201"}, "McGilloway2000": false, "Pereira2000": false, "PolzinWaibel2000": false, "Douglas-Cowie2000": false, "France2000": {"names": "France, DJ; Shiavi, RG; Silverman, S; Silverman, M; Wilkes, DM", "full_names": "France, DJ; Shiavi, RG; Silverman, S; Silverman, M; Wilkes, DM", "title": "Acoustical properties of speech as indicators of depression and suicidal risk", "journal": "IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING", "language": "English", "publication_type": "Article", "author_keywords": "acoustics; depression; multivariate; prediction; psychomotor; speech; suicide", "keywords_plus": "MAJOR DEPRESSION; BEHAVIOR; DISORDERS; SYMPTOMS", "abstract": "Acoustic properties of speech have previously been identified as possible cues to depression, and there is evidence that certain vocal parameters may be used further to objectively discriminate between depressed and suicidal speech. Studies were performed to analyze and compare the speech acoustics of separate male and female samples comprised of normal individuals and individuals carrying diagnoses of depression and high-risk, near-term suicidality. The female sample consisted of ten control subjects, 17 dysthymic patients, and 21 major depressed patients. The male sample contained 24 control subjects, 21 major depressed patients, and 22 high-risk suicidal patients. Acoustic analyses of voice fundamental frequency (F-0), amplitude modulation (AM), formants, and power distribution were performed on speech samples extracted from audio recordings collected from the sample members. Multivariate feature and discriminant analyses were performed on feature vectors representing the members of the control and disordered classes. Features derived from the formant and power spectral density measurements were found to be the best discriminators of class membership in both the male and female studies. AM features emerged as strong class discriminators of the male classes. Features describing F-0 were generally ineffective discriminators in both studies. The results support theories that identify psychomotor disturbances as central elements in depression and suicidality.", "affiliation": "L3 Commun, Salt Lake City, UT 84103 USA; Vanderbilt Univ, Dept Biomed Engn, Nashville, TN 37240 USA; Yale Univ, Sch Med, Dept Psychiat, Wilton, CT 06897 USA; Yale Psychiat Inst, Wilton, CT 06897 USA; Norwalk Hosp, Dept Pediat, Norwalk, CT 06883 USA; Vanderbilt Univ, Dept Elect & Comp Engn, Nashville, TN 37240 USA", "author_information": "France, DJ (reprint author), L3 Commun, 475 9th Ave, Salt Lake City, UT 84103 USA.", "email": NaN, "citation_count_WOS": 46, "wos_usage_count_180": 1, "wos_usage_count_2013": 22, "journal_abbr": "IEEE Trans. Biomed. Eng.", "month_day": "JUL", "year": 2000, "volume": 47, "issue": 7, "category1": "Engineering, Biomedical", "category2": "Engineering", "DOI": "10.1109/10.846676"}, "Walker2001": false, "Devillers2002": false, "FernandezPicard2003": false, "LeeNarayanan2003": false, "Yacoub2003": false, "McMahon2003": false, "Cowie2004": false, "Yildirim2004": false, "Tsuzuki2004": false, "Yu2004": false, "Zhang2004": false, "Lee2005": false, "Liscombe2005": false, "AlmSproat2005": false, "Ai2006": {"names": "Ai, H; Litman, DJ; Forbes-Riley, K; Rotaru, M; Tetreault, J; Purandare, A", "full_names": "Ai, Hua; Litman, Diane J.; Forbes-Riley, Kate; Rotaru, Mihai; Tetreault, Joel; Purandare, Amruta", "title": "Using System and User Performance Features to Improve Emotion Detection in Spoken Tutoring Dialogs", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotional speech; emotion detection; spoken dialog systems", "keywords_plus": NaN, "abstract": "In this study, we incorporate automatically obtained system/user performance features into machine learning experiments to detect student emotion in computer tutoring dialogs. Our results show a relative improvement of 2.7% on classification accuracy and 8.08% on Kappa over using standard lexical, prosodic, sequential, and identification features. This level of improvement is comparable to the performance improvement shown in previous studies by applying dialog acts or lexical-/prosodic-/discourse- level contextual features.", "affiliation": "[Ai, Hua; Litman, Diane J.; Purandare, Amruta] Univ Pittsburgh, Intelligent Syst Program, 210 S Bouquet, Pittsburgh, PA 15260 USA; [Forbes-Riley, Kate; Tetreault, Joel] Univ Pittsburgh, Learning Res & Dev Ctr, Pittsburgh, PA 15260 USA; [Rotaru, Mihai] Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA", "author_information": "Ai, H (reprint author), Univ Pittsburgh, Intelligent Syst Program, 210 S Bouquet, Pittsburgh, PA 15260 USA.", "email": "hua3@pitt.edu; dlitman@pitt.edu; forbesk@pitt.edu; mir25@pitt.edu; tetreaul@pitt.edu; adp22@pitt.edu", "citation_count_WOS": 17, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Clavel2006": false, "Devillers2006": false, "Lee2006": {"names": "Lee, S; Bresch, E; Adams, J; Kazemzadeh, A; Narayanan, S", "full_names": "Lee, Sungbok; Bresch, Erik; Adams, Jason; Kazemzadeh, Abe; Narayanan, Shrikanth", "title": "A Study of Emotional Speech Articulation using a Fast Magnetic Resonance Imaging Technique", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion; speech production; magnetic resonance imaging", "keywords_plus": NaN, "abstract": "A recently developed fast MR imaging system is utilized for a study of emotional speech production. Speech utterances and corresponding mid-sagittal vocal tract images are simultaneously acquired by the MRI system. Neutral, angry, sad and happy emotions are simulated by a male American English speaker. The MRI system and analysis results are described in this report. In general articulation is found to be more active in terms of the rate of vocal tract shaping and the ranges of spectral parameter values in emotional speech. It is confirmed that angry speech is characterized by wider and faster vocal tract shaping. Moreover, angry speech shows the more prominent usage of the pharyngeal region than any other emotions. It is also observed that the average vocal tract length above the false vocal folds varies as a function of emotion and that happy speech exhibit relatively shorter length than other emotions. It is likely that this is due to the elevation of the larynx and that may facilitate the higher pitch and larger pitch range manipulation to encode happy emotional quality by the speaker.", "affiliation": "[Lee, Sungbok; Bresch, Erik; Adams, Jason; Kazemzadeh, Abe; Narayanan, Shrikanth] Univ So Calif, Viterbi Sch Engn, SAIL, Los Angeles, CA 90089 USA", "author_information": "Lee, S (reprint author), Univ So Calif, Viterbi Sch Engn, SAIL, Los Angeles, CA 90089 USA.", "email": "sungbokl@usc.edu", "citation_count_WOS": 10, "wos_usage_count_180": 0, "wos_usage_count_2013": 2, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Kumar2006": {"names": "Kumar, R; Rose, CP; Litman, DJ", "full_names": "Kumar, Rohit; Rose, Carolyn P.; Litman, Diane J.", "title": "Identification of Confusion and Surprise in Spoken Dialog using Prosodic Features", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion detection; realistic settings; data collection methodology; self-report", "keywords_plus": NaN, "abstract": "Sensitivity to a user's emotional state offers promise in improving the state of the art in spoken dialog systems. In this work, we attempt to detect the speaker's states of confusion and surprise using prosodic features from his/her utterances. We have collected a corpus of utterances in realistic settings using an experimental methodology aimed at eliciting confusion and surprise from users. Classification experiments have yielded up to a 27.2% improvement over baseline performance using F0 and power features. We achieved the greatest success at classification of emotions that were most successfully elicited.", "affiliation": "[Kumar, Rohit; Rose, Carolyn P.] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA; [Litman, Diane J.] Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA", "author_information": "Kumar, R (reprint author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.", "email": "rohitk@cs.cmu.edu; cprose@cs.cmu.edu; litman@cs.pitt.edu", "citation_count_WOS": 10, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Neiberg2006": {"names": "Neiberg, D; Elenius, K; Laskowski, K", "full_names": "Neiberg, Daniel; Elenius, Kjell; Laskowski, Kornel", "title": "Emotion Recognition in Spontaneous Speech Using GMMs", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "Automatic detection of emotions has been evaluated using standard Mel-frequency Cepstral Coefficients, MFCCs, and a variant, MFCC-low, calculated between 20 and 300 Hz, in order to model pitch. Also plain pitch features have been used. These acoustic features have all been modeled by Gaussian mixture models, GMMs, on the frame level. The method has been tested on two different corpora and languages; Swedish voice controlled telephone services and English meetings. The results indicate that using GMMs on the frame level is a feasible technique for emotion classification. The two MFCC methods have similar performance, and MFCC-low outperforms the pitch features. Combining the three classifiers significantly improves performance.", "affiliation": "[Neiberg, Daniel; Elenius, Kjell] KTH, Dept Speech Mus & Hearing, Stockholm, Sweden; [Laskowski, Kornel] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA", "author_information": "Neiberg, D (reprint author), KTH, Dept Speech Mus & Hearing, Stockholm, Sweden.", "email": "neiberg@speech.kth.se; kjell@speech.kth.se; kornel@cs.cmu.edu", "citation_count_WOS": 17, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Tepperman2006": false, "Klasmeyer2000": false, "SchererCeschi1997": {"names": "Scherer, KR; Ceschi, G", "full_names": "Scherer, KR; Ceschi, G", "title": "Lost luggage: A field study of emotion antecedent appraisal", "journal": "MOTIVATION AND EMOTION", "language": "English", "publication_type": "Article", "author_keywords": NaN, "keywords_plus": "ATTRIBUTIONAL ANALYSIS; COGNITIVE APPRAISAL; DIMENSIONS; PATTERNS; ANGER; GUILT; MODEL", "abstract": "One hundred twelve airline passengers reporting their luggage lost to the baggage retrieval service in a major international airport were interviewed after their interaction with an airline agent. Participants were asked to rate their emotional state before and after the interaction with the agent and to provide information on how they had appraised the situation. The data are interpreted with respect to (1) type and intensity of the emotions felt in this situation, (2) appraisal theory predictions of emotion elicitation and differentiation, and (3) emotional change in the course of the interaction following reappraisal of the situation.", "affiliation": "Univ Geneva, Dept Psychol, CH-1227 Geneva, Switzerland", "author_information": "Scherer, KR (reprint author), Univ Geneva, Dept Psychol, 9 Route Dr, CH-1227 Geneva, Switzerland.", "email": NaN, "citation_count_WOS": 61, "wos_usage_count_180": 0, "wos_usage_count_2013": 6, "journal_abbr": "Motiv. Emot.", "month_day": "SEP", "year": 1997, "volume": 21, "issue": 3, "category1": "Psychology, Experimental; Psychology, Social", "category2": "Psychology", "DOI": "10.1023/A:1024498629430"}, "Nogueiras2001": false, "Batliner2004a": false, "BraunKaterbow2005": false, "Shigeno1998": false, "Chung2000": false, "Razak2005": {"names": "Razak, AA; Komiya, R; Abidin, MIZ", "full_names": "Razak, AA; Komiya, R; Abidin, MIZ", "title": "Comparison between fuzzy and NN method for speech emotion recognition", "journal": "Third International Conference on Information Technology and Applications, Vol 1, Proceedings", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion recognition speech processing; feature extraction; neural network; fuzzy model", "keywords_plus": NaN, "abstract": "This paper discusses an approach towards automatic recognition of emotion in speech which is adopted into a system named Voice Driven Emotion Recognizer Mobile Phone (VDERM). First, a design for the emotion recognizer is proposed. LPC analysis algorithm has been used for the speech emotion feature extraction. A total of 18 speech features have been selected to represent each emotion. A database consisting of emotional Malay and English, male and female voice samples have been developed for training and recognition purposes, Two recognition methods namely neural network and fuzzy model have been experimented and compared. The results show that both methods have their own advantage and disadvantage in application to emotion recognition. A recognition rate of up 60% is achievable by using these computer methods which is sufficient based on the recognition rate achieved by human.", "affiliation": "Multimedia Univ, Fac Informat Technol, Cyberjaya 63100, Selangor, Malaysia", "author_information": "Razak, AA (reprint author), Multimedia Univ, Fac Informat Technol, Cyberjaya 63100, Selangor, Malaysia.", "email": NaN, "citation_count_WOS": 10, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Software Engineering; Telecommunications", "category2": "Computer Science; Telecommunications", "DOI": NaN}, "Laukka2004": false, "GharavianAhadi2005": false, "V\u00e4yrynen2003": false, "JohnstoneScherer1999": {"names": "Ilves, M; Surakka, V; Vanhala, T", "full_names": "Ilves, Mirja; Surakka, Veikko; Vanhala, Toni", "title": "The Effects of Emotionally Worded Synthesized Speech on the Ratings of Emotions and Voice Quality", "journal": "AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PT I", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "Emotions; speech synthesis; facial expression; voice quality", "keywords_plus": "MEMORY", "abstract": "The present research investigated how the verbal content of synthetic messages affects participants' emotional responses and the ratings of voice quality. 28 participants listened to emotionally worded sentences produced by a monotonous and a prosodic tone of voice while the activity of corrugator supercilii facial muscle was measured. Ratings of emotions and voice quality were also collected. The results showed that the ratings of emotions were significantly affected by the emotional contents of the sentences. The prosodic tone of voice evoked more emotion-relevant ratings of arousal than the monotonous voice. Corrugator responses did not seem to reflect emotional reactions. Interestingly, the quality of the same voice was rated higher when the content of the sentences was positive as compared to the neutral and negative sentences. Thus, the emotional content of the spoken messages can be used to regulate users' emotions and to evoke positive feelings about the voices.", "affiliation": "[Ilves, Mirja; Surakka, Veikko; Vanhala, Toni] Univ Tampere, Sch Informat Sci, Tampere Unit Comp Human Interact, FI-33014 Tampere, Finland", "author_information": "Ilves, M (reprint author), Univ Tampere, Sch Informat Sci, Tampere Unit Comp Human Interact, FI-33014 Tampere, Finland.", "email": "mirja.ilves@cs.uta.fi; veikko.surakka@cs.uta.fi; toni.vanhala@vtt.fi", "citation_count_WOS": 17, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2011, "volume": 6974, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "category2": "Computer Science", "DOI": NaN}, "Chateau2004": false, "Abrilian2005": false, "BellerMarty2006": false, "TolkmittScherer1986": false, "BanseScherer1996": {"names": "Banse, R; Scherer, KR", "full_names": "Banse, R; Scherer, KR", "title": "Acoustic profiles in vocal emotion expression", "journal": "JOURNAL OF PERSONALITY AND SOCIAL PSYCHOLOGY", "language": "English", "publication_type": "Article", "author_keywords": NaN, "keywords_plus": "SPEAKER AFFECT; VOICE QUALITY; CUES; SPEECH", "abstract": "Professional actors' portrayals of 14 emotions varying in intensity and valence were presented to judges. The results on decoding replicate earlier findings on the ability of judges to infer vocally expressed emotions with much-better-than-chance accuracy, including consistently found differences in the recognizability of different emotions. A total of 224 portrayals were subjected to digital acoustic analysis to obtain profiles of vocal parameters for different emotions. The data suggest that vocal parameters not only index the degree of intensity typical for different emotions but also differentiate valence or quality aspects. The data are also used to test theoretical predictions on vocal patterning based on the component process model of emotion (K. R. Scherer, 1986). Although most hypotheses are supported, some need to be revised on the basis of the empirical evidence. Discriminant analysis and jackknifing show remarkably high hit rates and patterns of confusion that closely mirror those found for listener-judges.", "affiliation": "UNIV GENEVA,DEPT PSYCHOL,CH-1227 GENEVA,SWITZERLAND; HUMBOLDT UNIV BERLIN,DEPT PSYCHOL,BERLIN,GERMANY", "author_information": NaN, "email": NaN, "citation_count_WOS": 51, "wos_usage_count_180": 6, "wos_usage_count_2013": 81, "journal_abbr": "J. Pers. Soc. Psychol.", "month_day": "MAR", "year": 1996, "volume": 70, "issue": 3, "category1": "Psychology, Social", "category2": "Psychology", "DOI": "10.1037/0022-3514.70.3.614"}, "Dellaert1996": false, "Klasmeyer1996": false, "Alter2000": false, "Batliner2000": false, "Schr\u00f6der2000": false, "Kehrein2001": false, "Wahlster2006": false, "Tato2002": false, "Batliner2004b": false, "Burkhardt2005": false, "Schuller2005": false, "KimAndr\u00e92006": {"names": "Kim, J; Andre, E", "full_names": "Kim, Jonghwa; Andre, Elisabeth", "title": "Emotion recognition using physiological and speech signal in short-term observation", "journal": "PERCEPTION AND INTERACTIVE TECHNOLOGIES, PROCEEDINGS", "language": "English", "publication_type": "Article; Proceedings Paper", "author_keywords": NaN, "keywords_plus": NaN, "abstract": "Recently, there has been a significant amount of work on the recognition of emotions from visual, verbal or physiological information. Most approaches to emotion recognition so far concentrate, however, on a single modality while work on the integration of multimodal information, in particular on fusing physiological signals with verbal or visual data, is scarce. In this paper, we analyze various methods for fusing physiological and vocal information and compare the recognition results of the bimodal recognition approach with the results of the unimodal approach.", "affiliation": "Univ Augsburg, Inst Comp Sci, D-8900 Augsburg, Germany", "author_information": "Kim, J (reprint author), Univ Augsburg, Inst Comp Sci, D-8900 Augsburg, Germany.", "email": "Kim@informatik.uni-augsburg.de; Andre@informatik.uni-augsburg.de", "citation_count_WOS": 17, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": 4021, "issue": NaN, "category1": "Computer Science, Artificial Intelligence", "category2": "Computer Science", "DOI": NaN}, "Fakotakis2004": false, "Amir2000": false, "Nicholson1999": false, "Oudeyer2003": false, "Hirose2004": false, "Iwai2004": false, "Takahash2005": false, "Nisimura2006": {"names": "Nisimura, R; Omae, S; Kawahara, H; Irino, T", "full_names": "Nisimura, Ryuichi; Omae, Souji; Kawahara, Hideki; Irino, Toshio", "title": "Analyzing Dialogue Data for Real-World Emotional Speech Classification", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotional speech; classification; factor analysis; dialogue system; real world", "keywords_plus": NaN, "abstract": "In order to obtain an understanding of the user's emotion in human-machine dialogues, an analysis of dialogical utterances in the real world was performed. This work comprises three major steps. (1) The actual conditions of 16 basic emotions were evaluated using Japanese child voices, which were collected through the field test of the public spoken dialogue system. (2) Two factors were derived by a factor analysis. The factors were defined as fundamental psychological factors representing \"delightful\" and \"hateable\" emotions. (3) The relationships between the factors and the physical acoustic features were investigated to establish a capability to sense a user's mental state for the dialogue system. In the experimental discriminations between the delightful and hateable emotions, a correct rate of 98.8% was achieved in classifying child's utterances by the SVM (Support Vector Machine) with 11 acoustic features.", "affiliation": "[Nisimura, Ryuichi; Omae, Souji; Kawahara, Hideki; Irino, Toshio] Wakayama Univ, Fac Syst Engn, Wakayama, Japan", "author_information": "Nisimura, R (reprint author), Wakayama Univ, Fac Syst Engn, Wakayama, Japan.", "email": "nisimura@sys.wakayama-u.ac.jp", "citation_count_WOS": 12, "wos_usage_count_180": 0, "wos_usage_count_2013": 1, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Kim2004b": false, "Kim2005": {"names": "Kim, EH; Hyun, KH; Kwak, YK", "full_names": "Kim, EH; Hyun, KH; Kwak, YK", "title": "Robust emotion recognition feature, frequency range of meaningful signal", "journal": "2005 IEEE International Workshop on Robot and Human Interactive Communication (RO-MAN)", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion recognition; frequency range of meaningful signal; emotional group; emotional sate; HRI", "keywords_plus": NaN, "abstract": "Although the literature in emotion recognition from voice emphasizes that the recognition of emotions is generally classified in term of primary (or basic) emotions. However, they fail to explain the rationale for their classification. In addition, for the more exact recognition, more features to classify emotion are needed. But there are only a few features such as energy, pitch, and tempo. Hence, rather than using primary emotions, we classify emotions in emotional groups that have the same emotional state. We also propose a new feature called the frequency range of meaningful signal for emotion recognition from voice. In contrast to other features, this feature is independent of the magnitude of a speech signal and it is robust in a noisy environment. We also confirm the usefulness of this proposed feature through recognition experiments.", "affiliation": "Korea Adv Inst Sci & Technol, Dept Mech Engn, Taejon, South Korea", "author_information": "Kim, EH (reprint author), Korea Adv Inst Sci & Technol, Dept Mech Engn, Taejon, South Korea.", "email": NaN, "citation_count_WOS": 10, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2005, "volume": NaN, "issue": NaN, "category1": "Computer Science, Cybernetics; Robotics", "category2": "Computer Science; Robotics", "DOI": NaN}, "Makarova2002": false, "Montero1999": false, "Iriondo2000": false, "AbelinAllwood2000": false, "Karlsson1999": false, "Matsunaga2006": {"names": "Matsunaga, S; Sakaguchi, S; Yamashita, M; Miyahara, S; Nishitani, S; Shinohara, K", "full_names": "Matsunaga, S.; Sakaguchi, S.; Yamashita, M.; Miyahara, S.; Nishitani, S.; Shinohara, K.", "title": "Emotion Detection in Infants' Cries Based on a Maximum Likelihood Approach", "journal": "INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5", "language": "English", "publication_type": "Proceedings Paper", "author_keywords": "emotion detection; infants' cry; acoustic model", "keywords_plus": "CRY", "abstract": "This paper proposes a new procedure based on a maximum likelihood approach using hidden Markov models to detect infants' emotions through their cries. Our procedure uses stochastic acoustic models for each kind of emotion. The acoustic models are generated using infant's cries that are labeled segmentally according to their acoustic features. The procedure detects segment sequences with the highest likelihood among all kinds of emotions. The results of our preliminary recognition experiments on two emotions using three types of segment labeling show that the proposed procedure is applicable to emotion detection in an infant's cry and that the detailed transcription of acoustic segments is useful. In this paper, using detailed transcriptions, we broaden the experiments to include five emotions. Assuming the judgment of each infant's mother to be correct, we compared the result of the experiment and that of a subjective opinion test. We conducted the opinion test with the help of three child-rearing experts. Emotion recognition using the proposed procedure displays a favorable comparison with the judgment of our experts, showing the validity of the proposed procedure.", "affiliation": "[Matsunaga, S.; Sakaguchi, S.; Yamashita, M.; Miyahara, S.] Nagasaki Univ, Dept Comp & Informat Sci, Nagasaki, Japan; [Nishitani, S.; Shinohara, K.] Nagasaki Univ, Dept Translat Med Sci, Nagasaki, Japan", "author_information": "Matsunaga, S (reprint author), Nagasaki Univ, Dept Comp & Informat Sci, Nagasaki, Japan.", "email": "mat@cis.nagasaki-u.ac.jp", "citation_count_WOS": 7, "wos_usage_count_180": 0, "wos_usage_count_2013": 0, "journal_abbr": NaN, "month_day": NaN, "year": 2006, "volume": NaN, "issue": NaN, "category1": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "category2": "Computer Science", "DOI": NaN}, "Navas2004": false, "Cowie1999": false, "Batliner2004": false, "Kim2004": false}